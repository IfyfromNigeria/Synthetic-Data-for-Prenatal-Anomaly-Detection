# -*- coding: utf-8 -*-
"""L00187665_Synthethic Eval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZVOhpDaYz60U2zTce-9gKaNPGPrGhHSs

# LINK TO THE DATASET
"""

# LINK TO THE DATASET
'''https://data.mendeley.com/datasets/n2rbrb9t4f/1'''

"""# DATA LOADING AND PREPROCESSING"""

# LINK GOOGLE DRIVE
from google.colab import drive
drive.mount('/content/drive')

# DO NOT RUN ON COLAB, ONLY JUPYTER
import os
import zipfile

folder = 'Downloads/Thesis Datasets'
output = 'Thesis_Datasets'
os.makedirs(output, exist_ok = True)

for zip_name in os.listdir(folder):
    if zip_name.endswith('.zip'):
        zip_path = os.path.join(folder, zip_name)
        extract_folder = os.path.join(output, os.path.splitext(zip_name)[0])

        # Create subfolder for each zip
        os.makedirs(extract_folder, exist_ok=True)

        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_folder)
            print(f"Extracted: {zip_name} â†’ {extract_folder}")

import os
import pandas as pd
# Path to Shenzhen dataset
root_path = '/content/drive/MyDrive/Thesis/Thesis Datasets/Shenzhen Dataset/Shenzhen_NT'

# Walk through all directories and files
for dirpath, dirnames, filenames in os.walk(root_path):
    print(f'Directory: {dirpath}')
    for dirname in dirnames:
        print(f'  Subdirectory: {dirname}')
    for filename in filenames:
        print(f'  File: {filename}')
    print("-" * 50)

import pandas as pd
annotation_path = '/content/drive/MyDrive/Thesis/Thesis Datasets/Shenzhen Dataset/Shenzhen_NT/Dataset for Fetus Framework/ObjectDetection.xlsx'
annotations = pd.read_excel(annotation_path)

annotations.head()

# COMBINE US IMAGES FROM ONLY THE STANDARD FOLDERS
import os
import pandas as pd

# BASE DIRECTORY
base_dir = "/content/drive/MyDrive/Thesis/Thesis Datasets/Shenzhen Dataset/Shenzhen_NT/Dataset for Fetus Framework/Dataset for Fetus Framework"

# Step 2: Define paths to the four "Standard" folders
standard_folders = [
    os.path.join(base_dir, "External Test Set/Standard"),
    os.path.join(base_dir, "Internal Test Set/Standard"),
    os.path.join(base_dir, "Set1-Training&Validation Sets CNN/Standard"),
    os.path.join(base_dir, "Set2-Training&Validation Sets ANN Scoring system/Standard")
]

# Step 3: Load all image filenames from these folders
shenzhen_standard_image_paths = []
for folder in standard_folders:
    for fname in os.listdir(folder):
        if fname.lower().endswith(".png"):
            shenzhen_standard_image_paths.append({
                "fname": fname,
                "image_path": os.path.join(folder, fname)
            })

df_images = pd.DataFrame(shenzhen_standard_image_paths)

shenzhen_standard_image_paths

df_images.count()

# PAIR ANNOTATIONS WITH STANDARD US FROM SHENZHEN DATASET
relevant_structures = ['NT', 'nasal bone']
filtered_annotations = annotations[annotations['structure'].str.lower().isin([s.lower() for s in relevant_structures])]

# Merge based on file name
Shenzhen_merged = pd.merge(df_images, filtered_annotations, on='fname', how='inner')

# Check result
print(f"Merged dataset shape: {Shenzhen_merged.shape}")
print(Shenzhen_merged.head())

shenzhen_merged = pd.DataFrame(Shenzhen_merged)

shenzhen_merged.head()

nt = sum(shenzhen_merged['structure']=='NT')
print(nt)
nasal = sum(shenzhen_merged['structure']=='nasal bone')
nasal

shenzhen_merged.to_csv('shenzhen_merged.csv', index = False)



"""# PREPROCESSING FOR NT IMAGES"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import cv2
from albumentations import (
    Compose, Rotate, HorizontalFlip, VerticalFlip, RandomBrightnessContrast, Resize,
    Normalize, ShiftScaleRotate, GaussianBlur
)
from albumentations.pytorch import ToTensorV2
import torch
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import matplotlib.patches as patches

from google.colab import drive
drive.mount('/content/drive')

shenzhen_merged.head()

# PREPROCESSING FOR NT IMAGES
# Filter for NT structures and calculate NT thickness
nt_df = shenzhen_merged[shenzhen_merged['structure'] == 'NT'].copy()

# Calculate NT thickness from vertical dimension (h_max - h_min)
nt_df['nt_thickness_px'] = nt_df['h_max'] - nt_df['h_min']
nt_df['nt_thickness_mm'] = nt_df['nt_thickness_px'] * 0.1  # Convert to mm
nt_df['label'] = nt_df['nt_thickness_mm'].apply(lambda x: 1 if x > 3.5 else 0)  # 1=anomalous, 0=normal

# Calculate bounding box dimensions for cropping
nt_df['width'] = nt_df['w_max'] - nt_df['w_min']
nt_df['height'] = nt_df['h_max'] - nt_df['h_min']

# Split into train and temp
df_train, df_temp = train_test_split(nt_df, test_size=0.25, stratify=nt_df['label'], random_state=16)

# Split temp into validation and test
df_val, df_test = train_test_split(df_temp, test_size=0.4, stratify=df_temp['label'], random_state=16)

nt_df.head()

print(sum(nt_df['label']==1))
print(sum(nt_df['label']==0))

# Split into train and temp
df_train, df_temp = train_test_split(nt_df, test_size=0.25, stratify=nt_df['label'], random_state=16)

# Split temp into validation and test
df_val, df_test = train_test_split(df_temp, test_size=0.4, stratify=df_temp['label'], random_state=16)

test_dataset = df_test.copy()

# IMAGE PREPROCESSING USING CLAHE (USED BY RESHI ET AL., 2024)
# CLAHE CONFIG.
def apply_clahe(gray_img, clip_limit=2.0, grid_size=(8, 8)):
    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=grid_size)
    return clahe.apply(gray_img)

# Visualization function
def visualize_sample(image, bbox, thickness, label):
    """Visualize image with bounding box and thickness information"""
    # Convert to RGB if grayscale
    if len(image.shape) == 2:
        vis_img = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
    else:
        vis_img = image.copy()

    # Create figure
    fig, ax = plt.subplots(1, figsize=(10, 8))
    ax.imshow(vis_img)

    # Create bounding box rectangle
    w_min, h_min, w_max, h_max = bbox
    width = w_max - w_min
    height = h_max - h_min
    rect = patches.Rectangle(
        (w_min, h_min), width, height,
        linewidth=2, edgecolor='r', facecolor='none'
    )
    ax.add_patch(rect)

    # Add text info
    status = "ANOMALOUS" if label == 1 else "NORMAL"
    plt.title(f"NT Thickness: {thickness:.2f}mm ({status})\n"
             f"BBox: x={w_min}-{w_max}, y={h_min}-{h_max}", fontsize=12)
    plt.axis('off')
    plt.show()

# Dataset class with correct coordinate handling
class NTDataset(Dataset):
    def __init__(self, df, transform=None, phase='train'):
        self.df = df
        self.transform = transform
        self.phase = phase

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img_path = row['image_path']

        # Read image
        image = cv2.imread(img_path)
        if image is None:
            raise FileNotFoundError(f"Image not found at {img_path}")

        # Get bounding box coordinates
        x_min = row['w_min']
        x_max = row['w_max']
        y_min = row['h_min']
        y_max = row['h_max']

        # Convert to integers
        x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)

        # Boundary checks
        height, width = image.shape[:2]
        x_min = max(0, x_min)
        y_min = max(0, y_min)
        x_max = min(width, x_max)
        y_max = min(height, y_max)

        # Ensure valid bounding box
        if x_max <= x_min or y_max <= y_min:
            # Fallback to entire image if invalid bbox
            nt_crop = image.copy()
            print(f"Warning: Invalid bounding box in {img_path} - using full image")
        else:
            # Crop NT region
            nt_crop = image[y_min:y_max, x_min:x_max]

        # Handle empty crops
        if nt_crop.size == 0:
            nt_crop = image.copy()
            print(f"Warning: Empty crop in {img_path} - using full image")

        # Convert to grayscale
        gray = cv2.cvtColor(nt_crop, cv2.COLOR_BGR2GRAY) if len(nt_crop.shape) == 3 else nt_crop

        # Apply CLAHE
        clahe_img = apply_clahe(gray)

        # Convert to 3-channel for model compatibility
        clahe_rgb = cv2.cvtColor(clahe_img, cv2.COLOR_GRAY2RGB)

        # Apply transformations
        if self.transform:
            transformed = self.transform(image=clahe_rgb)
            image_tensor = transformed['image']
        else:
            # Fallback to simple resize if no transform
            resized = cv2.resize(clahe_rgb, (224, 224))
            image_tensor = torch.from_numpy(resized).permute(2, 0, 1).float() / 255.0

        # Get label and thickness
        label = row['label']

        # Visualize first sample for verification
        if idx == 0 and self.phase == 'train':
            print(f"Sample - Original dimensions: {image.shape}")
            print(f"NT region: w_min={x_min}, h_min={y_min}, w_max={x_max}, h_max={y_max}")

        return image_tensor, label

# Enhanced augmentation for ResNet18
def get_train_transform():
    return Compose([
        Rotate(limit=12, border_mode=cv2.BORDER_CONSTANT, fill=0, p=0.8),
        HorizontalFlip(p=0.5),
        VerticalFlip(p=0.3),
        ShiftScaleRotate(
            shift_limit=0.08,
            scale_limit=0.15,
            rotate_limit=8,
            border_mode=cv2.BORDER_CONSTANT,
            fill=0,
            p=0.8
        ),
        RandomBrightnessContrast(
            brightness_limit=0.25,
            contrast_limit=0.25,
            p=0.6
        ),
        GaussianBlur(blur_limit=(3, 5), p=0.3),
        Resize(224, 224),
        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2()
    ])

def get_val_transform():
    return Compose([
        Resize(224, 224),  # Same as training
        Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet stats
                  std=[0.229, 0.224, 0.225]),
        ToTensorV2()
    ])

# Create datasets and dataloaders
train_dataset = NTDataset(
    df_train,
    transform=get_train_transform(),
    phase='train'
)

val_dataset = NTDataset(
    df_val,
    transform=get_val_transform(),
    phase='val'
)

BATCH_SIZE = 16
train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=0,
    pin_memory=True
)

val_loader = DataLoader(
    val_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=0,
    pin_memory=True
)

# Dataset statistics
print(f"Total NT samples: {len(nt_df)}")
print(f"Training samples: {len(train_dataset)}")
print(f"Validation samples: {len(val_dataset)}")
print(f"Testing samples: {len(test_dataset)}")
print(f"Normal samples: {sum(nt_df['label'] == 0)}")
print(f"Anomalous samples: {sum(nt_df['label'] == 1)}")
print(f"Anomaly ratio: {sum(nt_df['label'] == 1)/len(nt_df):.2f}")

"""# CLASSIFICATION ON REAL US IMAGES WITH EFFICIENTNET-B0"""

!pip install efficientnet_pytorch
# Upgrade albumentations
!pip install albumentations==1.0.3
!pip install torchvision==0.10.0
!pip install pydantic==1.8.2

import pandas as pd
import os
import numpy as np
from sklearn.model_selection import train_test_split
import cv2
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from albumentations import (
    Compose, Rotate, HorizontalFlip, VerticalFlip, RandomBrightnessContrast, Resize,
    Normalize, ShiftScaleRotate, GaussianBlur
)
from albumentations.pytorch import ToTensorV2
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from efficientnet_pytorch import EfficientNet
import albumentations as A
from albumentations.pytorch import ToTensorV2
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, roc_auc_score, accuracy_score
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from tqdm import tqdm
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data import Dataset, DataLoader

# set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

# Define model and training parameters
NUM_EPOCHS = 40
BATCH_SIZE = 16
LEARNING_RATE = 1e-4
SAVE_DIR = "/content/drive/MyDrive/Thesis/real_model_artifacts"
os.makedirs(SAVE_DIR, exist_ok=True)
HISTORY_FILE = os.path.join(SAVE_DIR, 'real_training_history.json')
MODEL_PATH = os.path.join(SAVE_DIR, 'real_best_model_enet.pth')

nt_df = pd.read_csv('/content/real_nt.csv')

# Split into train and temp
df_train, df_temp = train_test_split(nt_df, test_size=0.25, stratify=nt_df['label'], random_state=16)

# Split temp into validation and test
df_val, df_test = train_test_split(df_temp, test_size=0.4, stratify=df_temp['label'], random_state=16)

def apply_clahe(image, clipLimit=2.0, tileGridSize=(8, 8)):
    """Applies Contrast Limited Adaptive Histogram Equalization (CLAHE)."""
    clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)
    return clahe.apply(image)

# Enhanced augmentations for EfficientNet
def get_train_transform_en():
    return A.Compose([
        A.RandomResizedCrop(height=224, width=224, scale=(0.7, 1.0), ratio=(0.8, 1.2), p=0.5),
        A.Rotate(limit=15, border_mode=cv2.BORDER_CONSTANT, value=0, p=0.8),
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.3),
        A.ShiftScaleRotate(
            shift_limit=0.1,
            scale_limit=0.2,
            rotate_limit=12,
            border_mode=cv2.BORDER_CONSTANT,
            value=0,
            p=0.8
        ),
        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.7),
        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05, p=0.5),
        A.GaussianBlur(blur_limit=(3, 7), p=0.3),
        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.2),
        A.CoarseDropout(max_holes=8, max_height=16, max_width=16, fill_value=0, p=0.3),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2()
    ])

def get_val_transform_en():
    return A.Compose([
        A.Resize(height=256, width=256),
        A.CenterCrop(height=224, width=224),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2()
    ])

def get_test_transform_en():
    return A.Compose([
        A.Resize(height=256, width=256),
        A.CenterCrop(height=224, width=224),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2()
    ])

class NTDataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df.reset_index(drop=True)
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img_path = row['image_path']

        # Read image
        image = cv2.imread(img_path)
        if image is None:
            raise FileNotFoundError(f"Image not found at {img_path}")

        # Resize, CLAHE, and convert to RGB
        image_resized = cv2.resize(image, (224, 224))
        gray = cv2.cvtColor(image_resized, cv2.COLOR_BGR2GRAY)
        clahe_img = apply_clahe(gray)
        clahe_rgb = cv2.cvtColor(clahe_img, cv2.COLOR_GRAY2RGB)

        # Apply augmentation and normalization
        if self.transform:
            augmented = self.transform(image=clahe_rgb)
            image_tensor = augmented['image']
        else:
            image_tensor = torch.tensor(clahe_rgb).permute(2, 0, 1).float() / 255.0

        label = torch.tensor(row['label'], dtype=torch.float32)
        return image_tensor, label

# Create datasets and dataloaders
train_dataset = NTDataset(
    df_train,
    transform=get_train_transform_en()
)

val_dataset = NTDataset(
    df_val,
    transform=get_val_transform_en()
)

test_dataset = NTDataset(
    df_test,
    transform=get_test_transform_en()
)

BATCH_SIZE = 16
train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=2,
    pin_memory=True
    )

val_loader = DataLoader(
    val_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=2,
    pin_memory=True
    )

test_loader = DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=2,
    pin_memory=True
    )

# Dataset statistics
print(f"Total NT samples: {len(nt_df)}")
print(f"Training samples: {len(train_dataset)}")
print(f"Validation samples: {len(val_dataset)}")
print(f"Testing samples: {len(test_dataset)}")
print(f"Normal samples: {sum(nt_df['label'] == 0)}")
print(f"Anomalous samples: {sum(nt_df['label'] == 1)}")
print(f"Anomaly ratio: {sum(nt_df['label'] == 1)/len(nt_df):.2f}")

print("Displaying a sample image from the DataLoader...")
# Get a single batch of data from the DataLoader
images, labels = next(iter(train_loader))

# Select the first image and label from the batch
image_tensor = images[0]
label_tensor = labels[0]

# Un-normalize the image tensor for proper visualization
mean = np.array([0.485, 0.456, 0.406])
std = np.array([0.229, 0.224, 0.225])
image_np = image_tensor.permute(1, 2, 0).numpy()
image_np = std * image_np + mean
image_np = np.clip(image_np, 0, 1)

# Map the label to a descriptive string
label_map = {0.0: 'Normal', 1.0: 'Anomalous'}
label_str = label_map.get(label_tensor.item(), 'Unknown')

# Display the image and label
plt.figure(figsize=(6, 6))
plt.imshow(image_np)
plt.title(f"Image from DataLoader\nLabel: {label_str}")
plt.axis('off')
plt.show()

print("\nImage displayed successfully!")

class NTClassifier(nn.Module):
    def __init__(self):
        super(NTClassifier, self).__init__()
        self.base = EfficientNet.from_pretrained('efficientnet-b0')
        # Freeze all parameters in the base model
        for param in self.base.parameters():
            param.requires_grad = False

        # Replace the final fully connected layer with an identity module
        num_features = self.base._fc.in_features
        self.base._fc = nn.Identity()

        # Custom classifier on top of the base model
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(num_features, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Dropout(0.3),
            nn.Linear(256, 1)
        )

    def forward(self, x):
        features = self.base(x)
        return self.classifier(features)

# TRAINING HELPER FUNCTIONS
def unfreeze_layers(model, epoch):
    """Gradually unfreezes layers of the EfficientNet model for fine-tuning."""
    if epoch == 3:
        # Unfreeze classifier layers
        for param in model.classifier.parameters():
            param.requires_grad = True
        print("Unfroze classifier layers")

    elif epoch == 8:
        # Unfreeze top 3 blocks of the base model
        for name, param in model.base.named_parameters():
            if 'blocks.5' in name or 'blocks.6' in name or 'blocks.7' in name:
                param.requires_grad = True
        print("Unfroze top 3 blocks")

    elif epoch == 15:
        # Unfreeze all layers of the model
        for param in model.parameters():
            param.requires_grad = True
        print("Unfroze all layers")

# MAIN TRAINING FUNCTIONS
def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs):
    """
    Main training loop for the model.
    Saves the best model based on validation loss and training history,
    computes and stores AUC and F1-score for each epoch.
    """
    history = {
        "train_loss": [],
        "val_loss": [],
        "train_acc": [],
        "val_acc": [],
        "train_f1": [],
        "val_f1": [],
        "train_auc": [],
        "val_auc": [],
    }
    best_val_loss = float('inf')

    for epoch in range(1, num_epochs + 1):
        unfreeze_layers(model, epoch)
        print(f"Epoch {epoch}/{num_epochs}")

        # --- Training phase ---
        model.train()
        train_loss = 0.0
        train_preds = []
        train_labels = []
        train_probs = []
        total_train = 0

        for images, labels in tqdm(train_loader, desc="Training"):
            images, labels = images.to(device), labels.float().unsqueeze(1).to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * images.size(0)

            probs = torch.sigmoid(outputs)
            preds = (probs > 0.5).float()

            train_preds.extend(preds.detach().cpu().numpy().flatten())
            train_labels.extend(labels.detach().cpu().numpy().flatten())
            train_probs.extend(probs.detach().cpu().numpy().flatten())
            total_train += labels.size(0)

        avg_train_loss = train_loss / total_train
        train_accuracy = accuracy_score(train_labels, train_preds)
        train_f1 = f1_score(train_labels, train_preds)
        train_auc = roc_auc_score(train_labels, train_probs)

        history['train_loss'].append(avg_train_loss)
        history['train_acc'].append(train_accuracy)
        history['train_f1'].append(train_f1)
        history['train_auc'].append(train_auc)

        # --- Validation phase ---
        model.eval()
        val_loss = 0.0
        val_preds = []
        val_labels = []
        val_probs = []
        total_val = 0
        with torch.no_grad():
            for images, labels in tqdm(val_loader, desc="Validation"):
                images, labels = images.to(device), labels.float().unsqueeze(1).to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)

                val_loss += loss.item() * images.size(0)

                probs = torch.sigmoid(outputs)
                preds = (probs > 0.5).float()

                val_preds.extend(preds.detach().cpu().numpy().flatten())
                val_labels.extend(labels.detach().cpu().numpy().flatten())
                val_probs.extend(probs.detach().cpu().numpy().flatten())
                total_val += labels.size(0)

        avg_val_loss = val_loss / total_val
        val_accuracy = accuracy_score(val_labels, val_preds)
        val_f1 = f1_score(val_labels, val_preds)
        val_auc = roc_auc_score(val_labels, val_probs)

        history['val_loss'].append(avg_val_loss)
        history['val_acc'].append(val_accuracy)
        history['val_f1'].append(val_f1)
        history['val_auc'].append(val_auc)

        print(f"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Train F1: {train_f1:.4f}, Train AUC: {train_auc:.4f}")
        print(f"Validation Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, Val AUC: {val_auc:.4f}")

        # Save the best model based on validation loss
        if avg_val_loss < best_val_loss:
            print(f"Validation loss improved from {best_val_loss:.4f} to {avg_val_loss:.4f}. Saving model...")
            best_val_loss = avg_val_loss
            torch.save({
                'epoch': epoch,
                'real_model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': best_val_loss
            }, MODEL_PATH)

    # Save training history to a JSON file
    with open(HISTORY_FILE, 'w') as f:
        json.dump(history, f)
    print("Training history saved.")

# VISUALIZATION FUNCTION
def plot_training_history(history_file):
    """Plots the training and validation loss, accuracy, F1-score, and AUC from a JSON file."""
    if not os.path.exists(history_file) or os.path.getsize(history_file) == 0:
        print(f"Error: The history file '{history_file}' does not exist or is empty.")
        return

    try:
        with open(history_file, 'r') as f:
            history = json.load(f)
    except json.JSONDecodeError:
        print(f"Error: The file '{history_file}' is not a valid JSON file. It might be corrupted.")
        return

    required_keys = ['train_loss', 'val_loss', 'train_acc', 'val_acc', 'train_f1', 'val_f1', 'train_auc', 'val_auc']
    if not all(key in history for key in required_keys):
        print("The history file is missing required data keys.")
        return

    epochs = range(1, len(history['train_loss']) + 1)

    plt.figure(figsize=(18, 12))

    # Plot Loss
    plt.subplot(2, 2, 1)
    plt.plot(epochs, history['train_loss'], 'b-o', label='Training Loss')
    plt.plot(epochs, history['val_loss'], 'r-o', label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    # Plot Accuracy
    plt.subplot(2, 2, 2)
    plt.plot(epochs, history['train_acc'], 'b-o', label='Training Accuracy')
    plt.plot(epochs, history['val_acc'], 'r-o', label='Validation Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)

    # Plot F1-Score
    plt.subplot(2, 2, 3)
    plt.plot(epochs, history['train_f1'], 'b-o', label='Training F1-Score')
    plt.plot(epochs, history['val_f1'], 'r-o', label='Validation F1-Score')
    plt.title('Training and Validation F1-Score')
    plt.xlabel('Epochs')
    plt.ylabel('F1-Score')
    plt.legend()
    plt.grid(True)

    # Plot AUC
    plt.subplot(2, 2, 4)
    plt.plot(epochs, history['train_auc'], 'b-o', label='Training AUC')
    plt.plot(epochs, history['val_auc'], 'r-o', label='Validation AUC')
    plt.title('Training and Validation AUC')
    plt.xlabel('Epochs')
    plt.ylabel('AUC')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plot_path = os.path.join(SAVE_DIR, 'real_training_history_plot.png')
    plt.savefig(plot_path, dpi=300)
    plt.show()
    print(f"Training history plot saved to {plot_path}")

# VISUALIZATION OF PREDICTIONS BY THE MODEL
def visualize_predictions(results, num_samples=10):
    """
    Visualizes a selection of correct and incorrect predictions.
    """
    correct = [r for r in results if r['true_label'] == r['pred_label']]
    incorrect = [r for r in results if r['true_label'] != r['pred_label']]

    samples = (correct[:num_samples//2] if len(correct) >= num_samples//2 else correct) + \
              (incorrect[:num_samples//2] if len(incorrect) >= num_samples//2 else incorrect)

    if not samples:
        print("No samples to visualize.")
        return

    num_cols = 5
    num_rows = (len(samples) + num_cols - 1) // num_cols
    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 4 * num_rows))

    # Flatten the axes array for easier iteration
    if num_rows == 1:
        axes = [axes] if len(samples) <= 5 else axes
    else:
        axes = axes.flatten()

    for i, ax in enumerate(axes):
        if i >= len(samples):
            ax.axis('off')
            continue

        res = samples[i]

        # Load and preprocess the image
        img_path = res['image_path']
        if not os.path.exists(img_path):
            img = np.zeros((224, 224, 3), dtype=np.uint8)
            cv2.putText(img, "Dummy Image", (20, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        else:
            img = cv2.imread(img_path)
            if img is not None:
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            else:
                img = np.zeros((224, 224, 3), dtype=np.uint8)

        # Add text info
        status_map = {0.0: "NORMAL", 1.0: "ANOMALOUS"}
        true_status = status_map.get(res['true_label'], 'N/A')
        pred_status = status_map.get(res['pred_label'], 'N/A')

        color = "green" if res['true_label'] == res['pred_label'] else "red"

        ax.imshow(img)
        title = (f"True: {true_status}\nPred: {pred_status}\n"
                 f"Prob: {res['probability']:.2f}")
        ax.set_title(title, color=color, fontsize=10)
        ax.axis('off')

    plt.tight_layout()
    viz_path = os.path.join(SAVE_DIR, 'real_sample_predictions.png')
    plt.savefig(viz_path, dpi=300)
    plt.show()
    print(f"Prediction visualization saved to {viz_path}")

# EVALUATION FUNCTION
def evaluate_model(model, test_loader, df_test):
    """
    Evaluates the final model on the test set and returns results for visualization.
    """
    model.eval()
    model.to(device)

    all_preds = []
    all_labels = []
    all_probs = []

    print("Evaluating model on test set...")
    with torch.no_grad():
        for images, labels in tqdm(test_loader, desc="Evaluation"):
            images, labels = images.to(device), labels.float().unsqueeze(1).to(device)
            outputs = model(images)
            probs = torch.sigmoid(outputs)
            preds = (probs > 0.5).float()

            all_preds.extend(preds.cpu().numpy().flatten())
            all_labels.extend(labels.cpu().numpy().flatten())
            all_probs.extend(probs.cpu().numpy().flatten())

    # Calculate and print metrics
    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds)
    roc_auc = roc_auc_score(all_labels, all_probs)

    print("================== Evaluation Metrics ==================")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"AUC: {roc_auc:.4f}")
    print("======================================================")

    # Plot confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(6, 6))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.colorbar()
    plt.xticks([0, 1], ['Normal', 'Anomalous'])
    plt.yticks([0, 1], ['Normal', 'Anomalous'])

    thresh = cm.max() / 2
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, format(cm[i, j], 'd'),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()
    cm_path = os.path.join(SAVE_DIR, 'real_confusion_matrix.png')
    plt.savefig(cm_path, dpi=300)
    plt.show()
    print(f"Confusion matrix saved to {cm_path}")

    # Prepare results for visualization
    test_results = []
    for i in range(len(all_labels)):
        test_results.append({
            'image_path': df_test.iloc[i]['image_path'],
            'true_label': all_labels[i],
            'pred_label': all_preds[i],
            'probability': all_probs[i]
        })
    return test_results

if __name__ == "__main__":
    # Initialize the model, optimizer, and loss function
    model = NTClassifier().to(device)
    # Freeze all layers of the pre-trained base model initially
    for param in model.base.parameters():
        param.requires_grad = False

    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.BCEWithLogitsLoss()

    # Start the training process
    train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=NUM_EPOCHS)

    # Load the best model for evaluation
    try:
        checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['real_model_state_dict'])
        print("\nBest model loaded successfully for evaluation.")
    except FileNotFoundError:
        print("\nBest model not found. Cannot perform evaluation.")
        exit()

    # Evaluate the model on the test set
    test_results = evaluate_model(model, test_loader, df_test)

    # Visualize the predictions
    visualize_predictions(test_results, num_samples=10)

    # Plot the training history
    plot_training_history(HISTORY_FILE)

# GRAD-CAM
class EfficientNetGradCAM:
    """
    A class to compute and store Grad-CAM heatmaps for an EfficientNet model.
    """
    def __init__(self, model):
        self.model = model
        self.activations = None
        self.gradients = None

        # Register hooks on the last convolutional block of EfficientNet
        self.target_layer = self.model.base._blocks[-1]
        self.target_layer.register_forward_hook(self.save_activation)
        self.target_layer.register_backward_hook(self.save_gradient)

    def save_activation(self, module, input, output):
        self.activations = output

    def save_gradient(self, module, grad_input, grad_output):
        self.gradients = grad_output[0]

    def __call__(self, x):
        self.model.eval()
        self.model.zero_grad()

        # Forward pass to get output and populate activations
        output = self.model(x)

        # Backpropagate the gradient of the logit
        output.backward()

        activations = self.activations.detach()
        gradients = self.gradients.detach()

        # Pool the gradients to get weights
        weights = torch.mean(gradients, dim=[2, 3], keepdim=True)

        # Compute the CAM
        cam = torch.sum(weights * activations, dim=1, keepdim=True)
        cam = torch.relu(cam)

        # Normalize the CAM
        cam = cam - torch.min(cam)
        if torch.max(cam) > 0:
            cam = cam / torch.max(cam)
        else:
            cam = torch.zeros_like(cam)

        return cam.squeeze().cpu().numpy()

def visualize_gradcam(model, test_df, num_samples=5):
    """
    Generates and displays Grad-CAM visualizations for a few test samples.
    """
    gradcam = EfficientNetGradCAM(model)
    samples = test_df.sample(num_samples)

    # Set the model to evaluation mode
    model.eval()

    fig, axes = plt.subplots(1, num_samples, figsize=(20, 4))
    if num_samples == 1:
        axes = [axes]

    transform_for_cam = get_test_transform_en()

    for i, (_, row) in enumerate(samples.iterrows()):
        # Load and preprocess the image
        img_path = row['image_path']

        # Read image from file or create a dummy black image if not found
        if not os.path.exists(img_path):
            image = np.zeros((224, 224, 3), dtype=np.uint8)
            cv2.putText(image, "Dummy Image", (20, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        else:
            image = cv2.imread(img_path)
            if image is None:
                image = np.zeros((224, 224, 3), dtype=np.uint8)

        # This is your exact preprocessing pipeline, now correctly implemented
        image_resized = cv2.resize(image, (224, 224))
        gray = cv2.cvtColor(image_resized, cv2.COLOR_BGR2GRAY)
        clahe_img = apply_clahe(gray)
        original_img_rgb = cv2.cvtColor(clahe_img, cv2.COLOR_GRAY2RGB)

        # Apply the test transform for model input
        input_tensor = transform_for_cam(image=original_img_rgb)['image'].unsqueeze(0).to(device)

        # Get prediction
        with torch.no_grad():
            output = model(input_tensor)
            prob = torch.sigmoid(output).item()
            pred = 1 if prob > 0.5 else 0

        # Get CAM - we need gradients for this part
        input_tensor.requires_grad_(True)
        cam = gradcam(input_tensor)

        # Resize CAM to original image size for overlay
        cam_resized = cv2.resize(cam, (original_img_rgb.shape[1], original_img_rgb.shape[0]))
        heatmap = (cam_resized * 255).astype(np.uint8)
        heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

        # Alpha blend the heatmap and original image
        superimposed_img = cv2.addWeighted(original_img_rgb, 0.6, heatmap, 0.4, 0)

        # Plot the result
        axes[i].imshow(superimposed_img)
        status_map = {0.0: "NORMAL", 1.0: "ANOMALOUS"}
        true_status = status_map.get(row['label'], 'N/A')
        pred_status = status_map.get(pred, 'N/A')

        color = "green" if row['label'] == pred else "red"

        title = (f"True: {true_status} | Pred: {pred_status}\n"
                 f"Prob: {prob:.2f}")
        axes[i].set_title(title, color=color, fontsize=10)
        axes[i].axis('off')

    plt.tight_layout()
    viz_path = os.path.join(SAVE_DIR, 'real_efficientnet_gradcam.png')
    plt.savefig(viz_path, dpi=300)
    plt.show()
    print(f"Grad-CAM visualization saved to {viz_path}")

# --- Visualize Grad-CAM ---
if __name__ == "__main__":
    model = NTClassifier().to(device)

    try:
        checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['real_model_state_dict'])
        print("Best model loaded successfully for Grad-CAM visualization.")
    except FileNotFoundError:
        print("Best model not found. Cannot perform Grad-CAM visualization.")
        exit()

    visualize_gradcam(model, df_test, num_samples=5)

"""# IMAGE PREPARATION FOR IMAGE GENERATION"""

import pandas as pd
import numpy as np
import os
import cv2
from sklearn.model_selection import train_test_split

shenzhen_merged = pd.read_csv('/content/shenzhen_merged (1).csv')
nt_df = shenzhen_merged[shenzhen_merged['structure'] == 'NT'].copy()

# Calculate NT thickness from vertical dimension (h_max - h_min)
nt_df['nt_thickness_px'] = nt_df['h_max'] - nt_df['h_min']
nt_df['nt_thickness_mm'] = nt_df['nt_thickness_px'] * 0.1  # Convert to mm
nt_df['label'] = nt_df['nt_thickness_mm'].apply(lambda x: 1 if x > 3.5 else 0)  # 1=anomalous, 0=normal

nt_df.head()

nt_gans = nt_df[['image_path', 'label']]

nt_gans.shape

nt_gans.to_csv('nt_gans.csv')

"""# IMAGE GENERATION WITH WGAN-GP"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
import os
import cv2
import json
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import umap.umap_ as umap
from torchvision import transforms
from torchvision.models import inception_v3, Inception_V3_Weights
from torchvision.utils import save_image

IMG_SIZE = 64
IMG_CHANNELS = 1
NUM_CLASSES = 2  # 1=anomalous, 0=normal
LATENT_DIM = 100

BATCH_SIZE = 32

MODEL_SAVE_PATH = "C:/Users/student/OneDrive - Atlantic TU/Thesis_models/WGANS_MODELS"
IMAGE_SAVE_PATH = "C:/Users/student/OneDrive - Atlantic TU/Thesis_models/WGANS_IMAGES"
CSV_PATH = "shenzhen_merged.csv"
CHECKPOINT_PATH = os.path.join(MODEL_SAVE_PATH, "wgan_gp_checkpoint.pt")
LOSS_LOG_PATH = os.path.join(MODEL_SAVE_PATH, "training_loss_log.csv")


# WGAN-GP specific hyperparameters
LAMBDA_GP = 10  # Weight for the gradient penalty term
CRITIC_UPDATES = 5  # Number of times to train the Critic for each Generator update
IMAGE_SAVE_INTERVAL = 50

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# DATASET AND DATA LOADING
class WGANDataset(Dataset):
    """
    Custom PyTorch Dataset for GAN training.
    """
    def __init__(self, df, img_size=IMG_SIZE):
        self.df = df
        self.img_size = img_size
        self.last_original_size = (0, 0)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img_path = str(row['image_path'])
        label = row['label']

        try:
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            if img is None:
                raise FileNotFoundError(f"Image not found at {img_path}")

            H, W = img.shape[:2]

            h_min = int(np.clip(round(row['h_min']), 0, H - 1))
            w_min = int(np.clip(round(row['w_min']), 0, W - 1))
            h_max = int(np.clip(round(row['h_max']), h_min + 1, H))
            w_max = int(np.clip(round(row['w_max']), w_min + 1, W))

            cropped_img = img[h_min:h_max, w_min:w_max]

            if cropped_img.size == 0:
                return torch.zeros((IMG_CHANNELS, self.img_size, self.img_size)), torch.tensor(-1)

            cropped_img = cv2.resize(cropped_img, (self.img_size, self.img_size), interpolation=cv2.INTER_LINEAR)
            img_tensor = torch.tensor(cropped_img, dtype=torch.float32).unsqueeze(0)
            img_tensor = (img_tensor / 127.5) - 1.0

            label_tensor = torch.tensor(label, dtype=torch.long)

            return img_tensor, label_tensor

        except Exception as e:
            print(f"Error loading image {img_path}: {e}")
            return torch.zeros((IMG_CHANNELS, self.img_size, self.img_size)), torch.tensor(-1)

def prepare_data(csv_path):
    """Loads and splits the dataset from a CSV file."""
    try:
        shenzhen_merged = pd.read_csv(csv_path)
    except FileNotFoundError:
        print(f"Error: The CSV file was not found at {csv_path}. Please check the path.")
        return None, None, None

    nt_df = shenzhen_merged[shenzhen_merged['structure'] == 'NT'].copy()
    if nt_df.empty:
        print("Error: No 'NT' data found in the CSV. Cannot process.")
        return None, None, None

    nt_df['nt_thickness_px'] = nt_df['h_max'] - nt_df['h_min']
    nt_df['nt_thickness_mm'] = nt_df['nt_thickness_px'] * 0.1
    nt_df['label'] = nt_df['nt_thickness_mm'].apply(lambda x: 1 if x > 3.5 else 0)
    nt_df = nt_df[nt_df['image_path'].apply(lambda p: os.path.exists(str(p)))].copy()
    nt_df.rename(columns={'image_path': 'image_path'}, inplace=True)
    nt_df['label'] = nt_df['label'].astype(int)

    train_df, temp_df = train_test_split(nt_df, test_size=0.15, random_state=16)
    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=16)

    train_dataset = WGANDataset(train_df)
    val_dataset = WGANDataset(val_df)
    test_dataset = WGANDataset(test_df)

    return train_dataset, val_dataset, test_dataset

# GENERATOR
class Generator(nn.Module):
    def __init__(self, latent_dim, num_classes, img_channels=1, img_size=IMG_SIZE):
        super().__init__()
        self.latent_dim = latent_dim
        self.img_channels = img_channels
        self.img_size = img_size
        self.main = nn.Sequential(
            nn.ConvTranspose2d(latent_dim + num_classes, 512, 4, 1, 0, bias=False),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2, inplace=True),
            nn.ConvTranspose2d(64, img_channels, 4, 2, 1, bias=False),
            nn.Tanh()
        )
    def forward(self, z, labels):
        x = torch.cat([z.view(z.size(0), self.latent_dim, 1, 1),
                       labels.view(labels.size(0), NUM_CLASSES, 1, 1)], dim=1)
        img = self.main(x)
        return img

# DISCRIMINATOR
class Discriminator(nn.Module):
    def __init__(self, num_classes, img_channels=1, img_size=IMG_SIZE):
        super().__init__()
        self.img_size = img_size
        self.num_classes = num_classes
        self.main = nn.Sequential(
            nn.Conv2d(img_channels + num_classes, 64, 4, 2, 1, bias=False),
            nn.GroupNorm(8, 64),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 128, 4, 2, 1, bias=False),
            nn.GroupNorm(16, 128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, 4, 2, 1, bias=False),
            nn.GroupNorm(32, 256),
            nn.LeakyReLU(0.2, inplace=True)
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(256 * (self.img_size // 8) ** 2, 1)
        )
    def forward(self, img, labels):
        labels_reshaped = labels.view(labels.size(0), self.num_classes, 1, 1)
        labels_tiled = labels_reshaped.repeat(1, 1, self.img_size, self.img_size)
        x = torch.cat([img, labels_tiled], dim=1)
        features = self.main(x)
        # The Discriminator (now a Critic) outputs raw values, not probabilities.
        # So we do not use a Sigmoid activation here.
        output = self.classifier(features)
        return output.view(-1, 1)

# WGAN-GP
def calculate_gradient_penalty(critic, real_images, fake_images, labels, device):
    """
    Calculates the gradient penalty for WGAN-GP.
    """
    batch_size = real_images.size(0)
    alpha = torch.rand(batch_size, 1, 1, 1, device=device)

    # Linear interpolation between real and fake samples
    interpolates = alpha * real_images + (1 - alpha) * fake_images
    interpolates.requires_grad_(True)

    # Pass interpolated samples through the Critic
    interpolates_labels = labels
    critic_interpolates = critic(interpolates, interpolates_labels)

    # Calculate the gradient of the Critic's output with respect to the interpolated samples
    gradients = torch.autograd.grad(
        outputs=critic_interpolates,
        inputs=interpolates,
        grad_outputs=torch.ones(critic_interpolates.size(), device=device),
        create_graph=True,
        retain_graph=True,
    )[0]

    gradients = gradients.view(gradients.size(0), -1)

    # Calculate the L2 norm of the gradients
    gradient_norm = gradients.norm(2, dim=1)

    # Calculate the gradient penalty
    gradient_penalty = ((gradient_norm - 1) ** 2).mean()

    return gradient_penalty

"""**UTILITY FUNCTIONS**"""

def save_checkpoint(generator, discriminator, g_optimizer, d_optimizer, epoch):
    """
    Saves a checkpoint of the model and optimizers.
    """
    if not os.path.exists(MODEL_SAVE_PATH):
        os.makedirs(MODEL_SAVE_PATH)

    torch.save({
        'epoch': epoch,
        'generator_state_dict': generator.state_dict(),
        'discriminator_state_dict': discriminator.state_dict(),
        'g_optimizer_state_dict': g_optimizer.state_dict(),
        'd_optimizer_state_dict': d_optimizer.state_dict(),
    }, CHECKPOINT_PATH)
    print(f"Checkpoint saved to {CHECKPOINT_PATH}")

def load_checkpoint(generator, discriminator, g_optimizer, d_optimizer):
    """
    Loads a saved checkpoint.
    """
    if os.path.isfile(CHECKPOINT_PATH):
        print(f"Checkpoint found at '{CHECKPOINT_PATH}'! Loading model...")
        checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)
        generator.load_state_dict(checkpoint['generator_state_dict'])
        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])
        g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])
        d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])
        start_epoch = checkpoint['epoch']
        print("Model loaded successfully.")
        return start_epoch
    else:
        print("No checkpoint found. Starting from scratch.")
        return 0

def log_training_data(log_path, epoch, avg_d_loss, avg_g_loss):
    """
    Appends training loss data to a CSV file.
    """
    if not os.path.exists(os.path.dirname(log_path)):
        os.makedirs(os.path.dirname(log_path))

    # Write headers only if the file does not exist
    if not os.path.exists(log_path):
        with open(log_path, 'w') as f:
            f.write("Epoch,Critic Loss,Generator Loss\n")

    # Append the new loss data
    with open(log_path, 'a') as f:
        f.write(f"{epoch},{avg_d_loss:.4f},{avg_g_loss:.4f}\n")

def save_generated_images(generator, epoch, latent_vectors, labels, image_save_path):
    """
    Generates a batch of images and saves them to a file.
    """
    generator.eval()
    with torch.no_grad():
        generated_images = generator(latent_vectors, labels).cpu()

    if not os.path.exists(image_save_path):
        os.makedirs(image_save_path)

    save_image(
        generated_images,
        os.path.join(image_save_path, f'epoch_{epoch:04d}_generated.png'),
        nrow=8,
        normalize=True
    )
    generator.train()

def perform_latent_interpolation(generator, latent_dim, num_classes, num_steps=10, output_dir="interpolation_results"):
    """
    Performs latent space interpolation for each class and saves the results.
    """
    print("Starting Latent Space Interpolation...")
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    generator.eval()
    with torch.no_grad():
        for i in range(num_classes):
            print(f"Performing interpolation for class {i}...")

            z1 = torch.randn(1, latent_dim).to(device)
            z2 = torch.randn(1, latent_dim).to(device)

            interpolated_vectors = []
            for alpha in np.linspace(0, 1, num_steps):
                z_interp = z1 * (1 - alpha) + z2 * alpha
                interpolated_vectors.append(z_interp)

            labels = F.one_hot(torch.tensor([i] * num_steps, dtype=torch.long), num_classes).float().to(device)

            interpolated_images = generator(torch.cat(interpolated_vectors, dim=0), labels).cpu()

            save_image(
                interpolated_images,
                os.path.join(output_dir, f"interpolation_class_{i:02d}.png"),
                nrow=num_steps,
                normalize=True
            )
    print("Latent space interpolation complete. Results saved.")

def extract_features_in_batches(dataloader, feature_extractor, preprocess_transform, device):
    """
    Extracts features from a DataLoader in a memory-efficient way.
    """
    all_features = []
    with torch.no_grad():
        for images, _ in dataloader:
            # Prepare images for InceptionV3: resize and convert to 3 channels
            images_resized = F.interpolate(images, size=(299, 299), mode='bilinear', align_corners=False)
            images_rgb = images_resized.repeat(1, 3, 1, 1)

            # Apply normalization
            preprocessed_images = preprocess_transform(images_rgb)

            # Extract features and move to CPU to free up VRAM
            features = feature_extractor(preprocessed_images.to(device)).cpu().numpy()
            all_features.append(features)

    return np.vstack(all_features)

def create_tsne_plot(real_features, fake_features, output_path="tsne_plot.png"):
    """
    Creates and saves a UMAP plot for visualizing data distributions.
    """
    print("Creating UMAP plot...")

    all_features = np.vstack((real_features, fake_features))
    labels = np.array(["Real"] * len(real_features) + ["Synthetic"] * len(fake_features))

    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)
    embedding = reducer.fit_transform(all_features)

    plt.figure(figsize=(10, 8))

    plt.scatter(
        embedding[:len(real_features), 0],
        embedding[:len(real_features), 1],
        c='b',
        label='Real Images',
        alpha=0.6,
        s=10
    )

    plt.scatter(
        embedding[len(real_features):, 0],
        embedding[len(real_features):, 1],
        c='r',
        label='Synthetic Images',
        alpha=0.6,
        s=10
    )

    plt.title("UMAP Plot of Image Features", fontsize=16)
    plt.xlabel("UMAP Dimension 1")
    plt.ylabel("UMAP Dimension 2")
    plt.legend()
    plt.grid(True)

    output_dir = os.path.dirname(output_path)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)

    plt.savefig(output_path)
    print(f"UMAP plot saved to {output_path}")

def train_gan(num_epochs, start_epoch, train_loader, generator, discriminator, g_optimizer, d_optimizer, device,
              latent_dim, num_classes, critic_updates, fixed_noise, fixed_labels):
    """
    Training loop for WGAN-GP.
    """
    print("\nStarting WGAN-GP training...")
    generator.train()
    discriminator.train()

    total_steps = len(train_loader)

    for epoch in range(start_epoch, num_epochs):
        epoch_g_losses = []
        epoch_d_losses = []
        for i, (real_images, labels) in enumerate(train_loader):
            real_images = real_images.to(device)
            labels = F.one_hot(labels, num_classes).float().to(device)

            # Check for invalid data
            if torch.any(labels.sum(dim=1) == 0):
                continue

            # --- Train the Critic ---
            for _ in range(critic_updates):
                discriminator.zero_grad()

                # Generate fake images
                latent_vectors_fake = torch.randn(real_images.size(0), latent_dim, device=device)
                fake_images = generator(latent_vectors_fake, labels).detach()

                # Get scores for real and fake images
                real_scores = discriminator(real_images, labels)
                fake_scores = discriminator(fake_images, labels)

                # Calculate gradient penalty
                gradient_penalty = calculate_gradient_penalty(discriminator, real_images, fake_images, labels, device)

                # WGAN-GP loss
                critic_loss = fake_scores.mean() - real_scores.mean() + LAMBDA_GP * gradient_penalty

                critic_loss.backward()
                d_optimizer.step()

            # --- Train the Generator ---
            generator.zero_grad()

            latent_vectors_gen = torch.randn(real_images.size(0), latent_dim, device=device)
            fake_images_gen = generator(latent_vectors_gen, labels)

            gen_scores = discriminator(fake_images_gen, labels)

            # WGAN-GP Generator loss
            gen_loss = -gen_scores.mean()

            gen_loss.backward()
            g_optimizer.step()

            # --- Logging ---
            epoch_d_losses.append(critic_loss.item())
            epoch_g_losses.append(gen_loss.item())

            if (i + 1) % 10 == 0:
                print(f"Epoch [{epoch}/{num_epochs}], Step [{i+1}/{total_steps}], "
                      f"D Loss: {np.mean(epoch_d_losses):.4f}, G Loss: {np.mean(epoch_g_losses):.4f}")

        # Log the average loss for the entire epoch
        avg_d_loss = np.mean(epoch_d_losses)
        avg_g_loss = np.mean(epoch_g_losses)
        log_training_data(LOSS_LOG_PATH, epoch, avg_d_loss, avg_g_loss)

        save_checkpoint(generator, discriminator, g_optimizer, d_optimizer, epoch)

        # Save generated images at a specified interval
        if (epoch + 1) % IMAGE_SAVE_INTERVAL == 0:
            print("Labels for the generated images (0=normal, 1=anomalous):")
            print(fixed_labels_idx.view(8, 8))
            save_generated_images(generator, epoch, fixed_noise, fixed_labels, IMAGE_SAVE_PATH)


    print("Training complete.")

# EVALUATION FXN
def evaluate_gan(generator, discriminator, g_optimizer, d_optimizer, test_dataset):
    """
    Performs post-training evaluation steps, including latent interpolation and UMAP plotting.
    """
    print("\nStarting evaluation...")

    # Load the trained model for evaluation
    load_checkpoint(generator, discriminator, g_optimizer, d_optimizer)

    # Perform latent space interpolation first
    interpolation_output_dir = os.path.join(IMAGE_SAVE_PATH, "wgan_gp_interpolation_results")
    perform_latent_interpolation(generator, LATENT_DIM, NUM_CLASSES, output_dir=interpolation_output_dir)

    # Prepare data for UMAP plotting
    print("\nPreparing data for UMAP evaluation...")
    num_samples_to_plot = 1000 # Number of real and fake images to use for the plot

    # Initialize the feature extractor (InceptionV3)
    feature_extractor = inception_v3(weights=Inception_V3_Weights.DEFAULT)
    feature_extractor.fc = nn.Identity() # Remove the final classification layer
    feature_extractor.to(device).eval()

    # Preprocessing transform for InceptionV3
    preprocess_transform = transforms.Compose([
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

    # Extract features from real images
    print("Extracting features from real images...")
    real_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)
    real_features = extract_features_in_batches(real_dataloader, feature_extractor, preprocess_transform, device)[:num_samples_to_plot]

    # Generate fake images in batches and extract features
    print("Generating fake images and extracting features in batches...")

    fake_features = []
    with torch.no_grad():
        generator.eval()
        for i in range(0, num_samples_to_plot, BATCH_SIZE):
            current_batch_size = min(BATCH_SIZE, num_samples_to_plot - i)

            latent_vectors_fake = torch.randn(current_batch_size, LATENT_DIM, device=device)
            fake_labels_idx = torch.randint(0, NUM_CLASSES, (current_batch_size,), device=device, dtype=torch.long)
            fake_labels_onehot = F.one_hot(fake_labels_idx, NUM_CLASSES).float().to(device)

            fake_images_for_eval = generator(latent_vectors_fake, fake_labels_onehot)

            images_resized = F.interpolate(fake_images_for_eval, size=(299, 299), mode='bilinear', align_corners=False)
            images_rgb = images_resized.repeat(1, 3, 1, 1)
            preprocessed_images = preprocess_transform(images_rgb)

            features = feature_extractor(preprocessed_images).cpu().numpy()
            fake_features.append(features)

            torch.cuda.empty_cache()

    fake_features = np.vstack(fake_features)

    # Create the UMAP plot
    create_tsne_plot(real_features, fake_features, output_path=os.path.join(IMAGE_SAVE_PATH, "wgan_gp_umap_plot.png"))

    print("\nQualitative evaluation complete. Check your designated folders for the output images.")

if __name__ == "__main__":
    # Initialize models and optimizers
    netG = Generator(LATENT_DIM, NUM_CLASSES, img_channels=IMG_CHANNELS).to(device)
    netD = Discriminator(NUM_CLASSES, img_channels=IMG_CHANNELS).to(device)

    g_optimizer = torch.optim.Adam(netG.parameters(), lr=1e-4, betas=(0.5, 0.999))
    d_optimizer = torch.optim.Adam(netD.parameters(), lr=1e-4, betas=(0.5, 0.999))

    # --- IMPORTANT: Load checkpoint to resume training or start from scratch ---
    start_epoch = load_checkpoint(netG, netD, g_optimizer, d_optimizer)

    # Prepare data
    print("\nPreparing data...")
    train_dataset, val_dataset, test_dataset = prepare_data(CSV_PATH)
    if train_dataset is None or test_dataset is None:
        exit()

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

    # Generate fixed noise and labels for consistent image saving
    fixed_noise = torch.randn(64, LATENT_DIM, device=device)
    fixed_labels_idx = torch.randint(0, NUM_CLASSES, (64,), device=device, dtype=torch.long)
    fixed_labels = F.one_hot(fixed_labels_idx, NUM_CLASSES).float().to(device)

    # --- MAIN TRAINING LOOP ---
    train_gan(1501, start_epoch, train_loader, netG, netD, g_optimizer, d_optimizer, device,
              LATENT_DIM, NUM_CLASSES, CRITIC_UPDATES, fixed_noise, fixed_labels)

if __name__ == "__main__":
    evaluate_gan(netG, netD, g_optimizer, d_optimizer, test_dataset)

import matplotlib.pyplot as plt
import pandas as pd
import csv

LOSS_LOG_PATH = os.path.join(MODEL_SAVE_PATH, "training_loss_log.csv")

def plot_losses_from_history(history_path):
    """
    Plots the generator and discriminator losses from a CSV history file.

    Args:
        history_path (str): The path to the JSON file containing the training history.
    """
    history = []
     # Read CSV file as list of dictionaries
    with open(history_path, 'r') as file:
        reader = csv.DictReader(file)
        for row in reader:
            # Convert values to appropriate types (e.g., float or int)
            history.append({
                'Epoch': int(row['Epoch']),
                'Generator Loss': float(row['Generator Loss']),
                'Critic Loss': float(row['Critic Loss'])
            })

    # Extract losses and epochs from the list of dictionaries
    epochs = [entry['Epoch'] for entry in history]
    g_losses = [entry['Generator Loss'] for entry in history]
    d_losses = [entry['Critic Loss'] for entry in history]

    if not g_losses or not d_losses:
        print("Error: The history file is empty or formatted incorrectly.")
        return

    # Plotting the losses
    plt.figure(figsize=(12, 6))
    plt.plot(epochs, g_losses, label='Generator Loss', color='blue')
    plt.plot(epochs, d_losses, label='Discriminator Loss', color='red')

    plt.title('Generator and Discriminator Loss over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.savefig('Generator vs Discriminator Loss WGAN-GP.jpg', dpi=300)
    plt.show()

if __name__ == "__main__":
    # Define the path to your history file
    MODEL_SAVE_PATH = "C:/Users/student/OneDrive - Atlantic TU/Thesis_models/WGANS_MODELS"
    LOSS_LOG_PATH = os.path.join(MODEL_SAVE_PATH, "training_loss_log.csv")

    # Call the function to plot the losses
    plot_losses_from_history(LOSS_LOG_PATH)

"""**IMAGE GENERATION**"""

def generate_images_for_classification(generator, num_normal, num_anomalous,
                                       latent_dim, num_classes, device, output_dir="generated_dataset"):
    """
    Generates a specified number of normal and anomalous images for a classification dataset.
    Images are saved individually in a class-specific directory structure.
    """
    generator.eval()

    classes = {0: 'normal', 1: 'anomalous'}
    class_counts = {0: num_normal, 1: num_anomalous}

    print(f"\nGenerating {num_normal} normal and {num_anomalous} anomalous images for classification...")

    # Create output directories if they don't exist
    for class_id, class_name in classes.items():
        class_path = os.path.join(output_dir, class_name)
        if not os.path.exists(class_path):
            os.makedirs(class_path)

    with torch.no_grad():
        for class_id, class_name in classes.items():
            count = 0
            num_to_generate = class_counts[class_id]

            while count < num_to_generate:
                current_batch_size = min(BATCH_SIZE, num_to_generate - count)

                # Generate random noise and specific labels for the current batch
                latent_vectors = torch.randn(current_batch_size, latent_dim, device=device)
                labels = torch.full((current_batch_size, ), class_id, dtype=torch.long, device=device)
                labels_onehot = F.one_hot(labels, num_classes).float().to(device)

                # Generate images
                generated_images = generator(latent_vectors, labels_onehot)

                # Save each image individually
                for i in range(current_batch_size):
                    img = generated_images[i].cpu()
                    file_path = os.path.join(output_dir, class_name, f"synthetic_{class_name}_{count + i:04d}.png")
                    save_image(img, file_path, normalize=True)

                count += current_batch_size
                print(f"Generated {count}/{num_to_generate} images for class '{class_name}'.")

    print(f"\nImage generation for classification complete. Images saved to the '{output_dir}' directory.")
    generator.train()

# --- GENERATE IMAGES FOR CLASSIFICATION DATASET ---
    # Calculate the number of images to generate for each class to match the original ratio
if __name__ == "__main__":
    original_total = 614 + 496
    num_normal_to_generate = int(round((614 / original_total) * 1110))
    num_anomalous_to_generate = int(round((496 / original_total) * 1110))

    # Generate the images for your new dataset
    generate_images_for_classification(
        generator=netG,
        num_normal=num_normal_to_generate,
        num_anomalous=num_anomalous_to_generate,
        latent_dim=LATENT_DIM,
        num_classes=NUM_CLASSES,
        device=device,
        output_dir="C:/Users/student/OneDrive - Atlantic TU/Thesis_models/WGAN_generated_classification_dataset"
    )

"""**DATA PREPARATION FOR IMAGE CLASSIFICATION**"""

import pandas as pd
import os

# This should match the output directory from the previous script.
GENERATED_DATASET_PATH = "/content/drive/MyDrive/Thesis/Thesis Datasets/WGAN_generated_classification_dataset"

# The name of the CSV file to save the DataFrame to.
OUTPUT_CSV_PATH = "/content/drive/MyDrive/Thesis/Thesis Datasets/WGAN_generated_classification_dataset/synthetic_df.csv"

def create_dataset_dataframe(root_dir):
    """
    Traverses a directory, collects image file paths and labels, and
    stores them in a pandas DataFrame.
    """
    print(f"Scanning directory: {root_dir}...")

    file_paths = []
    labels = []

    # Define class labels based on folder names
    class_mapping = {'normal': 0, 'anomalous': 1}

    # Walk through the root directory and its subdirectories
    for dirpath, dirnames, filenames in os.walk(root_dir):
        # Determine the label based on the current subdirectory name
        # We only care about the last part of the path (the folder name)
        current_folder = os.path.basename(dirpath)

        # Check if the folder name is in our class mapping
        if current_folder in class_mapping:
            label = class_mapping[current_folder]

            # Add file path and label for each image found
            for filename in filenames:
                # Ensure we only process image files (png, jpg, jpeg, etc.)
                if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
                    full_path = os.path.join(dirpath, filename)
                    file_paths.append(full_path)
                    labels.append(label)

    # Create the pandas DataFrame
    dataset_df = pd.DataFrame({
        'image_path': file_paths,
        'label': labels
    })

    return dataset_df

if __name__ == "__main__":
    # Create the DataFrame
    synthetic_df = create_dataset_dataframe(GENERATED_DATASET_PATH)

    # Shuffle the DataFrame to mix up the classes
    synthetic_df = synthetic_df.sample(frac=1, random_state=16).reset_index(drop=True)

    # Print the head to confirm it looks correct
    print("\nDataFrame created successfully. First 5 rows:")
    print(synthetic_df.head())
    synthetic_df.to_csv('synthetic_df.csv', index = False)

synthetic_df.shape

"""**INTEGRITY CHECKS**"""

import pandas as pd
import cv2
import matplotlib.pyplot as plt
import os

SYNTHETIC_CSV_PATH = "/content/synthetic_df.csv"

def display_random_image_from_csv(csv_path):
    """
    Loads a DataFrame from a CSV, selects a random image path, and displays
    the image with its corresponding label.

    Args:
        csv_path (str): The path to the CSV file containing image metadata.
    """
    try:
        # Load the CSV file into a pandas DataFrame
        df = pd.read_csv(csv_path)

        if df.empty:
            print("Error: The DataFrame is empty. Please check the CSV file.")
            return

        # Select a single random row from the DataFrame
        random_row = df.sample(n=1).iloc[0]

        # Extract the image path and label from the selected row
        image_path = random_row['image_path']
        label = random_row['label']

        # Define a mapping for a more descriptive label
        label_mapping = {0: 'Normal', 1: 'Anomalous'}
        display_label = label_mapping.get(label, 'Unknown')

        # Check if the image file exists before trying to load it
        if not os.path.exists(image_path):
            print(f"Error: Image file not found at path: {image_path}")
            return

        # Load the image using OpenCV (cv2)
        # We use cv2.IMREAD_UNCHANGED to handle single-channel images correctly
        image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)

        if image is None:
            print(f"Error: Could not read image from path: {image_path}")
            return

        # Use matplotlib to display the image
        plt.figure(figsize=(6, 6))
        plt.imshow(image, cmap='gray' if image.ndim == 2 else None)
        plt.title(f"Random Image\nLabel: {display_label} ({label})", fontsize=16)
        plt.axis('off')  # Hide the axes
        plt.show()

        print(f"Successfully displayed image from: {image_path}")
        print(f"Image has label: {display_label} ({label})")

    except FileNotFoundError:
        print(f"Error: The CSV file was not found at {csv_path}. Please check the path.")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

if __name__ == "__main__":
    display_random_image_from_csv(SYNTHETIC_CSV_PATH)

"""# IMAGE CLASSIFICATION OF SYNTHETIC DATA WITH EFFICIENTNET-B0"""

!pip install efficientnet_pytorch
!pip install albumentations==1.0.3
!pip install torchvision==0.10.0
!pip install pydantic>=1.10.0, <=2.0.0

import pandas as pd
import os
import numpy as np
import cv2
import json
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from efficientnet_pytorch import EfficientNet
import albumentations as A
from albumentations.pytorch import ToTensorV2
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, accuracy_score
from tqdm import tqdm
from torch.cuda.amp import autocast, GradScaler
from torch.optim.lr_scheduler import ReduceLROnPlateau

# set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

# Define model and training parameters
NUM_EPOCHS = 40
BATCH_SIZE = 16
LEARNING_RATE = 1e-4
SAVE_DIR = "/content/drive/MyDrive/Thesis/synthetic_model_artifacts2"
os.makedirs(SAVE_DIR, exist_ok=True)
HISTORY_FILE = os.path.join(SAVE_DIR, 'syn_training_history.json')
MODEL_PATH = os.path.join(SAVE_DIR, 'syn_best_model_enet.pth')

df = pd.read_csv("/content/synthetic_df.csv")
type(df)

# Split into train and temp
df_train, df_temp = train_test_split(df, test_size=0.25, stratify=df['label'], random_state=16)

# Split temp into validation and test
df_val, df_test = train_test_split(df_temp, test_size=0.4, stratify=df_temp['label'], random_state=16)

def apply_clahe(image, clipLimit=2.0, tileGridSize=(8, 8)):
    """Applies Contrast Limited Adaptive Histogram Equalization (CLAHE)."""
    clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)
    return clahe.apply(image)

def get_train_transform_en():
    """Defines a robust set of augmentations for training."""
    return A.Compose([
        A.RandomResizedCrop(height=224, width=224, scale=(0.7, 1.0), ratio=(0.8, 1.2), p=0.5),
        A.Rotate(limit=15, border_mode=cv2.BORDER_CONSTANT, value=0, p=0.8),
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.3),
        A.ShiftScaleRotate(
            shift_limit=0.1,
            scale_limit=0.2,
            rotate_limit=12,
            border_mode=cv2.BORDER_CONSTANT,
            value=0,
            p=0.8
        ),
        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.7),
        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05, p=0.5),
        A.GaussianBlur(blur_limit=(3, 7), p=0.3),
        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.2),
        A.CoarseDropout(max_holes=8, max_height=16, max_width=16, fill_value=0, p=0.3),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2()
    ])

def get_val_transform_en():
    """Defines transformations for validation and test sets."""
    return A.Compose([
        A.Resize(height=256, width=256),
        A.CenterCrop(height=224, width=224),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2()
    ])

def get_test_transform_en():
    return A.Compose([
        A.Resize(height=256, width=256),
        A.CenterCrop(height=224, width=224),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2()
    ])

class SYNDataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df.reset_index(drop=True)
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img_path = row['image_path']

        # Read image
        image = cv2.imread(img_path)
        if image is None:
            raise FileNotFoundError(f"Image not found at {img_path}")

        # Resize, CLAHE, and convert to RGB
        image_resized = cv2.resize(image, (224, 224))
        gray = cv2.cvtColor(image_resized, cv2.COLOR_BGR2GRAY)
        clahe_img = apply_clahe(gray)
        clahe_rgb = cv2.cvtColor(clahe_img, cv2.COLOR_GRAY2RGB)

        # Apply augmentation and normalization
        if self.transform:
            augmented = self.transform(image=clahe_rgb)
            image_tensor = augmented['image']
        else:
            image_tensor = torch.tensor(clahe_rgb).permute(2, 0, 1).float() / 255.0

        label = torch.tensor(row['label'], dtype=torch.float32)
        return image_tensor, label

train_dataset = SYNDataset(df_train, transform=get_train_transform_en())
val_dataset = SYNDataset(df_val, transform=get_val_transform_en())
test_dataset = SYNDataset(df_test, transform=get_test_transform_en())


train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=2,
    pin_memory=True
    )

val_loader = DataLoader(
    val_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=2,
    pin_memory=True
    )

test_loader = DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=2,
    pin_memory=True
    )

    # Dataset statistics
print(f"Total samples: {len(df)}")
print(f"Training samples: {len(train_dataset)}")
print(f"Validation samples: {len(val_dataset)}")
print(f"Testing samples: {len(test_dataset)}")
print(f"Normal samples: {sum(df['label'] == 0)}")
print(f"Anomalous samples: {sum(df['label'] == 1)}")
print(f"Anomaly ratio: {sum(df['label'] == 1)/len(df):.2f}")

print("Displaying a sample image from the DataLoader...")
# Get a single batch of data from the DataLoader
images, labels = next(iter(train_loader))

# Select the first image and label from the batch
image_tensor = images[0]
label_tensor = labels[0]

# Un-normalize the image tensor for proper visualization
mean = np.array([0.485, 0.456, 0.406])
std = np.array([0.229, 0.224, 0.225])
image_np = image_tensor.permute(1, 2, 0).numpy()
image_np = std * image_np + mean
image_np = np.clip(image_np, 0, 1)

# Map the label to a descriptive string
label_map = {0.0: 'Normal', 1.0: 'Anomalous'}
label_str = label_map.get(label_tensor.item(), 'Unknown')

# Display the image and label
plt.figure(figsize=(6, 6))
plt.imshow(image_np)
plt.title(f"Image from DataLoader\nLabel: {label_str}")
plt.axis('off')
plt.show()

print("\nImage displayed successfully!")

class SYNClassifier(nn.Module):
    def __init__(self):
        super(SYNClassifier, self).__init__()
        self.base = EfficientNet.from_pretrained('efficientnet-b0')
        # Freeze all parameters in the base model
        for param in self.base.parameters():
            param.requires_grad = False

        # Replace the final fully connected layer with an identity module
        num_features = self.base._fc.in_features
        self.base._fc = nn.Identity()

        # Custom classifier on top of the base model
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(num_features, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Dropout(0.3),
            nn.Linear(256, 1)
        )

    def forward(self, x):
        features = self.base(x)
        return self.classifier(features)

# TRAINING HELPER FUNCTIONS
def unfreeze_layers(model, epoch):
    """Gradually unfreezes layers of the EfficientNet model for fine-tuning."""
    if epoch == 3:
        # Unfreeze classifier layers
        for param in model.classifier.parameters():
            param.requires_grad = True
        print("Unfroze classifier layers")

    elif epoch == 8:
        # Unfreeze top 3 blocks of the base model
        for name, param in model.base.named_parameters():
            if 'blocks.5' in name or 'blocks.6' in name or 'blocks.7' in name:
                param.requires_grad = True
        print("Unfroze top 3 blocks")

    elif epoch == 15:
        # Unfreeze all layers of the model
        for param in model.parameters():
            param.requires_grad = True
        print("Unfroze all layers")

# MAIN TRAINING FUNCTIONS
def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs):
    """
    Main training loop for the model.
    Saves the best model based on validation loss and training history,
    computes and stores AUC and F1-score for each epoch.
    """
    history = {
        "train_loss": [],
        "val_loss": [],
        "train_acc": [],
        "val_acc": [],
        "train_f1": [],
        "val_f1": [],
        "train_auc": [],
        "val_auc": [],
    }
    best_val_loss = float('inf')

    for epoch in range(1, num_epochs + 1):
        unfreeze_layers(model, epoch)
        print(f"Epoch {epoch}/{num_epochs}")

        # --- Training phase ---
        model.train()
        train_loss = 0.0
        train_preds = []
        train_labels = []
        train_probs = []
        total_train = 0

        for images, labels in tqdm(train_loader, desc="Training"):
            images, labels = images.to(device), labels.float().unsqueeze(1).to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * images.size(0)

            probs = torch.sigmoid(outputs)
            preds = (probs > 0.5).float()

            train_preds.extend(preds.detach().cpu().numpy().flatten())
            train_labels.extend(labels.detach().cpu().numpy().flatten())
            train_probs.extend(probs.detach().cpu().numpy().flatten())
            total_train += labels.size(0)

        avg_train_loss = train_loss / total_train
        train_accuracy = accuracy_score(train_labels, train_preds)
        train_f1 = f1_score(train_labels, train_preds)
        train_auc = roc_auc_score(train_labels, train_probs)

        history['train_loss'].append(avg_train_loss)
        history['train_acc'].append(train_accuracy)
        history['train_f1'].append(train_f1)
        history['train_auc'].append(train_auc)

        # --- Validation phase ---
        model.eval()
        val_loss = 0.0
        val_preds = []
        val_labels = []
        val_probs = []
        total_val = 0
        with torch.no_grad():
            for images, labels in tqdm(val_loader, desc="Validation"):
                images, labels = images.to(device), labels.float().unsqueeze(1).to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)

                val_loss += loss.item() * images.size(0)

                probs = torch.sigmoid(outputs)
                preds = (probs > 0.5).float()

                val_preds.extend(preds.detach().cpu().numpy().flatten())
                val_labels.extend(labels.detach().cpu().numpy().flatten())
                val_probs.extend(probs.detach().cpu().numpy().flatten())
                total_val += labels.size(0)

        avg_val_loss = val_loss / total_val
        val_accuracy = accuracy_score(val_labels, val_preds)
        val_f1 = f1_score(val_labels, val_preds)
        val_auc = roc_auc_score(val_labels, val_probs)

        history['val_loss'].append(avg_val_loss)
        history['val_acc'].append(val_accuracy)
        history['val_f1'].append(val_f1)
        history['val_auc'].append(val_auc)

        print(f"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Train F1: {train_f1:.4f}, Train AUC: {train_auc:.4f}")
        print(f"Validation Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, Val AUC: {val_auc:.4f}")

        # Save the best model based on validation loss
        if avg_val_loss < best_val_loss:
            print(f"Validation loss improved from {best_val_loss:.4f} to {avg_val_loss:.4f}. Saving model...")
            best_val_loss = avg_val_loss
            torch.save({
                'epoch': epoch,
                'syn_model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': best_val_loss
            }, MODEL_PATH)

    # Save training history to a JSON file
    with open(HISTORY_FILE, 'w') as f:
        json.dump(history, f)
    print("Training history saved.")

# VISUALIZATION FUNCTION
def plot_training_history(history_file):
    """Plots the training and validation loss, accuracy, F1-score, and AUC from a JSON file."""
    if not os.path.exists(history_file) or os.path.getsize(history_file) == 0:
        print(f"Error: The history file '{history_file}' does not exist or is empty.")
        return

    try:
        with open(history_file, 'r') as f:
            history = json.load(f)
    except json.JSONDecodeError:
        print(f"Error: The file '{history_file}' is not a valid JSON file. It might be corrupted.")
        return

    required_keys = ['train_loss', 'val_loss', 'train_acc', 'val_acc', 'train_f1', 'val_f1', 'train_auc', 'val_auc']
    if not all(key in history for key in required_keys):
        print("The history file is missing required data keys.")
        return

    epochs = range(1, len(history['train_loss']) + 1)

    plt.figure(figsize=(18, 12))

    # Plot Loss
    plt.subplot(2, 2, 1)
    plt.plot(epochs, history['train_loss'], 'b-o', label='Training Loss')
    plt.plot(epochs, history['val_loss'], 'r-o', label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    # Plot Accuracy
    plt.subplot(2, 2, 2)
    plt.plot(epochs, history['train_acc'], 'b-o', label='Training Accuracy')
    plt.plot(epochs, history['val_acc'], 'r-o', label='Validation Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)

    # Plot F1-Score
    plt.subplot(2, 2, 3)
    plt.plot(epochs, history['train_f1'], 'b-o', label='Training F1-Score')
    plt.plot(epochs, history['val_f1'], 'r-o', label='Validation F1-Score')
    plt.title('Training and Validation F1-Score')
    plt.xlabel('Epochs')
    plt.ylabel('F1-Score')
    plt.legend()
    plt.grid(True)

    # Plot AUC
    plt.subplot(2, 2, 4)
    plt.plot(epochs, history['train_auc'], 'b-o', label='Training AUC')
    plt.plot(epochs, history['val_auc'], 'r-o', label='Validation AUC')
    plt.title('Training and Validation AUC')
    plt.xlabel('Epochs')
    plt.ylabel('AUC')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plot_path = os.path.join(SAVE_DIR, 'syn_training_history_plot.png')
    plt.savefig(plot_path, dpi=300)
    plt.show()
    print(f"Training history plot saved to {plot_path}")

# VISUALIZATION OF PREDICTIONS BY THE MODEL
def visualize_predictions(results, num_samples=10):
    """
    Visualizes a selection of correct and incorrect predictions.
    """
    correct = [r for r in results if r['true_label'] == r['pred_label']]
    incorrect = [r for r in results if r['true_label'] != r['pred_label']]

    samples = (correct[:num_samples//2] if len(correct) >= num_samples//2 else correct) + \
              (incorrect[:num_samples//2] if len(incorrect) >= num_samples//2 else incorrect)

    if not samples:
        print("No samples to visualize.")
        return

    num_cols = 5
    num_rows = (len(samples) + num_cols - 1) // num_cols
    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 4 * num_rows))

    # Flatten the axes array for easier iteration
    if num_rows == 1:
        axes = [axes] if len(samples) <= 5 else axes
    else:
        axes = axes.flatten()

    for i, ax in enumerate(axes):
        if i >= len(samples):
            ax.axis('off')
            continue

        res = samples[i]

        # Load and preprocess the image
        img_path = res['image_path']
        if not os.path.exists(img_path):
            img = np.zeros((224, 224, 3), dtype=np.uint8)
            cv2.putText(img, "Dummy Image", (20, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        else:
            img = cv2.imread(img_path)
            if img is not None:
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            else:
                img = np.zeros((224, 224, 3), dtype=np.uint8)

        # Add text info
        status_map = {0.0: "NORMAL", 1.0: "ANOMALOUS"}
        true_status = status_map.get(res['true_label'], 'N/A')
        pred_status = status_map.get(res['pred_label'], 'N/A')

        color = "green" if res['true_label'] == res['pred_label'] else "red"

        ax.imshow(img)
        title = (f"True: {true_status}\nPred: {pred_status}\n"
                 f"Prob: {res['probability']:.2f}")
        ax.set_title(title, color=color, fontsize=10)
        ax.axis('off')

    plt.tight_layout()
    viz_path = os.path.join(SAVE_DIR, 'syn_sample_predictions.png')
    plt.savefig(viz_path, dpi=300)
    plt.show()
    print(f"Prediction visualization saved to {viz_path}")

# EVALUATION FUNCTION
def evaluate_model(model, test_loader, df_test):
    """
    Evaluates the final model on the test set and returns results for visualization.
    """
    model.eval()
    model.to(device)

    all_preds = []
    all_labels = []
    all_probs = []

    print("Evaluating model on test set...")
    with torch.no_grad():
        for images, labels in tqdm(test_loader, desc="Evaluation"):
            images, labels = images.to(device), labels.float().unsqueeze(1).to(device)
            outputs = model(images)
            probs = torch.sigmoid(outputs)
            preds = (probs > 0.5).float()

            all_preds.extend(preds.cpu().numpy().flatten())
            all_labels.extend(labels.cpu().numpy().flatten())
            all_probs.extend(probs.cpu().numpy().flatten())

    # Calculate and print metrics
    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds)
    roc_auc = roc_auc_score(all_labels, all_probs)

    print("================== Evaluation Metrics ==================")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"AUC: {roc_auc:.4f}")
    print("======================================================")

    # Plot confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(6, 6))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.colorbar()
    plt.xticks([0, 1], ['Normal', 'Anomalous'])
    plt.yticks([0, 1], ['Normal', 'Anomalous'])

    thresh = cm.max() / 2
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, format(cm[i, j], 'd'),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()
    cm_path = os.path.join(SAVE_DIR, 'efficientnet_confusion_matrix.png')
    plt.savefig(cm_path, dpi=300)
    plt.show()
    print(f"Confusion matrix saved to {cm_path}")

    # Prepare results for visualization
    test_results = []
    for i in range(len(all_labels)):
        test_results.append({
            'image_path': df_test.iloc[i]['image_path'],
            'true_label': all_labels[i],
            'pred_label': all_preds[i],
            'probability': all_probs[i]
        })
    return test_results

if __name__ == "__main__":
    # Initialize the model, optimizer, and loss function
    model = SYNClassifier().to(device)
    # Freeze all layers of the pre-trained base model initially
    for param in model.base.parameters():
        param.requires_grad = False

    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.BCEWithLogitsLoss()

    # Start the training process
    train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=NUM_EPOCHS)

    # Load the best model for evaluation
    try:
        checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['syn_model_state_dict'])
        print("\nBest model loaded successfully for evaluation.")
    except FileNotFoundError:
        print("\nBest model not found. Cannot perform evaluation.")
        exit()

    # Evaluate the model on the test set
    test_results = evaluate_model(model, test_loader, df_test)

    # Visualize the predictions
    visualize_predictions(test_results, num_samples=10)

    # Plot the training history
    plot_training_history(HISTORY_FILE)

# GRAD-CAM
class EfficientNetGradCAM:
    """
    A class to compute and store Grad-CAM heatmaps for an EfficientNet model.
    """
    def __init__(self, model):
        self.model = model
        self.activations = None
        self.gradients = None

        # Register hooks on the last convolutional block of EfficientNet
        self.target_layer = self.model.base._blocks[-1]
        self.target_layer.register_forward_hook(self.save_activation)
        self.target_layer.register_backward_hook(self.save_gradient)

    def save_activation(self, module, input, output):
        self.activations = output

    def save_gradient(self, module, grad_input, grad_output):
        self.gradients = grad_output[0]

    def __call__(self, x):
        self.model.eval()
        self.model.zero_grad()

        # Forward pass to get output and populate activations
        output = self.model(x)

        # Backpropagate the gradient of the logit
        output.backward()

        activations = self.activations.detach()
        gradients = self.gradients.detach()

        # Pool the gradients to get weights
        weights = torch.mean(gradients, dim=[2, 3], keepdim=True)

        # Compute the CAM
        cam = torch.sum(weights * activations, dim=1, keepdim=True)
        cam = torch.relu(cam)

        # Normalize the CAM
        cam = cam - torch.min(cam)
        if torch.max(cam) > 0:
            cam = cam / torch.max(cam)
        else:
            cam = torch.zeros_like(cam)

        return cam.squeeze().cpu().numpy()

def visualize_gradcam(model, test_df, num_samples=5):
    """
    Generates and displays Grad-CAM visualizations for a few test samples.
    """
    gradcam = EfficientNetGradCAM(model)
    samples = test_df.sample(num_samples)

    # Set the model to evaluation mode
    model.eval()

    fig, axes = plt.subplots(1, num_samples, figsize=(20, 4))
    if num_samples == 1:
        axes = [axes]

    transform_for_cam = get_test_transform_en()

    for i, (_, row) in enumerate(samples.iterrows()):
        # Load and preprocess the image
        img_path = row['image_path']

        # Read image from file or create a dummy black image if not found
        if not os.path.exists(img_path):
            image = np.zeros((224, 224, 3), dtype=np.uint8)
            cv2.putText(image, "Dummy Image", (20, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        else:
            image = cv2.imread(img_path)
            if image is None:
                image = np.zeros((224, 224, 3), dtype=np.uint8)

        # This is your exact preprocessing pipeline, now correctly implemented
        image_resized = cv2.resize(image, (224, 224))
        gray = cv2.cvtColor(image_resized, cv2.COLOR_BGR2GRAY)
        clahe_img = apply_clahe(gray)
        original_img_rgb = cv2.cvtColor(clahe_img, cv2.COLOR_GRAY2RGB)

        # Apply the test transform for model input
        input_tensor = transform_for_cam(image=original_img_rgb)['image'].unsqueeze(0).to(device)

        # Get prediction
        with torch.no_grad():
            output = model(input_tensor)
            prob = torch.sigmoid(output).item()
            pred = 1 if prob > 0.5 else 0

        # Get CAM - we need gradients for this part
        input_tensor.requires_grad_(True)
        cam = gradcam(input_tensor)

        # Resize CAM to original image size for overlay
        cam_resized = cv2.resize(cam, (original_img_rgb.shape[1], original_img_rgb.shape[0]))
        heatmap = (cam_resized * 255).astype(np.uint8)
        heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

        # Alpha blend the heatmap and original image
        superimposed_img = cv2.addWeighted(original_img_rgb, 0.6, heatmap, 0.4, 0)

        # Plot the result
        axes[i].imshow(superimposed_img)
        status_map = {0.0: "NORMAL", 1.0: "ANOMALOUS"}
        true_status = status_map.get(row['label'], 'N/A')
        pred_status = status_map.get(pred, 'N/A')

        color = "green" if row['label'] == pred else "red"

        title = (f"True: {true_status} | Pred: {pred_status}\n"
                 f"Prob: {prob:.2f}")
        axes[i].set_title(title, color=color, fontsize=10)
        axes[i].axis('off')

    plt.tight_layout()
    viz_path = os.path.join(SAVE_DIR, 'efficientnet_gradcam.png')
    plt.savefig(viz_path, dpi=300)
    plt.show()
    print(f"Grad-CAM visualization saved to {viz_path}")

# --- Visualize Grad-CAM ---
if __name__ == "__main__":
    model = SYNClassifier().to(device)

    try:
        checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['syn_model_state_dict'])
        print("Best model loaded successfully for Grad-CAM visualization.")
    except FileNotFoundError:
        print("Best model not found. Cannot perform Grad-CAM visualization.")
        exit()

    visualize_gradcam(model, df_test, num_samples=5)

"""# AUGMENTED/HYBRID DATASET"""

# Path to Shenzhen Dattaset
root_path = '/content/drive/MyDrive/Thesis/Thesis Datasets/Dataset for Fetus Framework'

# Walk through all directories and files
for dirpath, dirnames, filenames in os.walk(root_path):
    print(f'Directory: {dirpath}')
    for dirname in dirnames:
        print(f'  Subdirectory: {dirname}')
    for filename in filenames:
        print(f'  File: {filename}')
    print("-" * 50)

import cv2
import matplotlib.pyplot as plt
import random

shenzhen_path = "/content/drive/MyDrive/Thesis/Thesis Datasets/Dataset for Fetus Framework/Dataset for Fetus Framework/Dataset for Fetus Framework/Set1-Training-Validation Sets CNN/Standard"

# List files
shenzhen_files = os.listdir(shenzhen_path)

# Pick 5 random images to display
random_files = random.sample(shenzhen_files, 5)

# Display the images
for filename in random_files:
    img_path = os.path.join(shenzhen_path, filename)
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    plt.imshow(img)
    plt.title(f"Shenzhen - {filename}")
    plt.axis('off')
    plt.show()

import pandas as pd
# Path to the annotation file
annotation_path = "/content/drive/MyDrive/Thesis/Thesis Datasets/Dataset for Fetus Framework/Dataset for Fetus Framework/ObjectDetection.xlsx"

# Load the spreadsheet
annotations = pd.read_excel(annotation_path)

# Show the first few rows
annotations.head()

# ONLY THE STANDARD FOLDERS
import os
import pandas as pd

# BASE DIRECTORY
base_dir = "/content/drive/MyDrive/Thesis/Thesis Datasets/Dataset for Fetus Framework/Dataset for Fetus Framework/Dataset for Fetus Framework"

# Step 2: Define paths to the four "Standard" folders
standard_folders = [
    os.path.join(base_dir, "External Test Set/Standard"),
    os.path.join(base_dir, "Internal Test Set/Standard"),
    os.path.join(base_dir, "Set1-Training-Validation Sets CNN/Standard"),
    os.path.join(base_dir, "Set2-Training-Validation Sets ANN Scoring system/Standard")
]

# Step 3: Load all image filenames from these folders
shenzhen_standard_image_paths = []
for folder in standard_folders:
    for fname in os.listdir(folder):
        if fname.lower().endswith(".png"):
            shenzhen_standard_image_paths.append({
                "fname": fname,
                "image_path": os.path.join(folder, fname)
            })

df_images = pd.DataFrame(shenzhen_standard_image_paths)

# PAIR ANNOTATIONS WITH STANDARD US FROM SHENZHEN DATASET
relevant_structures = ['NT']
filtered_annotations = annotations[annotations['structure'].str.lower().isin([s.lower() for s in relevant_structures])]

# Merge based on file name
shenzhen_merged = pd.merge(df_images, filtered_annotations, on='fname', how='inner')

# Check result
print(f"Merged dataset shape: {shenzhen_merged.shape}")
print(shenzhen_merged.head())

# PREPROCESSING FOR NT IMAGES
# Filter for NT structures and calculate NT thickness
nt_df = shenzhen_merged.copy()

# Calculate NT thickness from vertical dimension (h_max - h_min)
nt_df['nt_thickness_px'] = nt_df['h_max'] - nt_df['h_min']
nt_df['nt_thickness_mm'] = nt_df['nt_thickness_px'] * 0.1  # Convert to mm
nt_df['label'] = nt_df['nt_thickness_mm'].apply(lambda x: 1 if x > 3.5 else 0)  # 1=anomalous, 0=normal

# Calculate bounding box dimensions for cropping
nt_df['width'] = nt_df['w_max'] - nt_df['w_min']
nt_df['height'] = nt_df['h_max'] - nt_df['h_min']

nt_df.head()

import cv2
import matplotlib.pyplot as plt

relevant_structures = ['NT']

filtered_annotations = annotations[annotations['structure'].str.lower().isin([s.lower() for s in relevant_structures])]
target_filename = '168.png'
image_path = f"/content/drive/MyDrive/Thesis/Thesis Datasets/Dataset for Fetus Framework/Dataset for Fetus Framework/Dataset for Fetus Framework/Set1-Training-Validation Sets CNN/Standard/{target_filename}"
image_annotations = filtered_annotations[filtered_annotations['fname'] == target_filename]

# Load and convert the image
img = cv2.imread(image_path)
if img is None:
    raise FileNotFoundError(f"Image not found at {image_path}")
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# Draw bounding boxes for relevant structures
for _, row in image_annotations.iterrows():
    structure = row['structure']
    h_min, w_min, h_max, w_max = int(row['h_min']), int(row['w_min']), int(row['h_max']), int(row['w_max'])

    # Draw rectangle
    cv2.rectangle(img, (w_min, h_min), (w_max, h_max), (0, 255, 0), 2)

    # Add label above the box
    label_position = (w_min, max(h_min - 10, 10))
    cv2.putText(img, structure, label_position, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

# Display the result
plt.figure(figsize=(8, 6))
plt.imshow(img)
plt.title(f"Filtered Annotations for {target_filename}")
plt.axis('off')
plt.show()

import cv2
import matplotlib.pyplot as plt
import os
import numpy as np

# Sample group of 3 unique images to demonstrate annotation application
filenames = np.random.choice(filtered_annotations['fname'].unique(), size = 5, replace = False)

# Define the base path for the images
image_base_path = "/content/drive/MyDrive/Thesis/Thesis Datasets/Dataset for Fetus Framework/Dataset for Fetus Framework/Dataset for Fetus Framework/Set1-Training-Validation Sets CNN/Standard"

for filename in filenames:
    # Get all annotations for this image
    image_data = filtered_annotations[filtered_annotations['fname'] == filename]
    # Construct the image path using the base path and the filename
    image_path = os.path.join(image_base_path, filename)

    # Load and check image
    img = cv2.imread(image_path)
    if img is None:
        print(f"Image not found: {image_path}")
        continue
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Draw all boxes for this image
    for _, row in image_data.iterrows():
        structure = row['structure']
        h_min, w_min, h_max, w_max = int(row['h_min']), int(row['w_min']), int(row['h_max']), int(row['w_max'])
        cv2.rectangle(img, (w_min, h_min), (w_max, h_max), (0, 255, 0), 2)
        label_position = (w_min, max(h_min - 10, 10))
        cv2.putText(img, structure, label_position, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

    # Show image with annotations
    plt.figure(figsize=(6, 6))
    plt.imshow(img)
    plt.title(f"Annotations for {filename}")
    plt.axis('off')
    plt.show()

import os
import pandas as pd

# Recursively find all image paths
search_root = "/content/drive/MyDrive/Thesis/Thesis Datasets/Dataset for Fetus Framework/Dataset for Fetus Framework/Dataset for Fetus Framework"
all_image_paths = {}

for root, dirs, files in os.walk(search_root):
    for file in files:
        if file.lower().endswith(('.png', '.jpg', '.jpeg')):
            all_image_paths[file] = os.path.join(root, file)

# Map each annotation row to its real image path
filtered_annotations['image_path'] = filtered_annotations['fname'].map(all_image_paths)

# Drop any rows where no matching image path was found
#filtered_annotations = filtered_annotations.dropna(subset=['image_path'])

# Show the result
filtered_annotations.head()

filtered_annotations.shape

import cv2
import matplotlib.pyplot as plt
import os
import numpy as np

filenames = np.random.choice(filtered_annotations['fname'].unique(), size = 5, replace = False)

for filename in filenames:
    # Get all annotations for this image
    image_data = filtered_annotations[filtered_annotations['fname'] == filename]
    image_path = image_data['image_path'].iloc[0]

    # Load and check image
    img = cv2.imread(image_path)
    if img is None:
        print(f"Image not found: {image_path}")
        continue
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Draw all boxes for this image
    for _, row in image_data.iterrows():
        structure = row['structure']
        h_min, w_min, h_max, w_max = int(row['h_min']), int(row['w_min']), int(row['h_max']), int(row['w_max'])
        cv2.rectangle(img, (w_min, h_min), (w_max, h_max), (0, 255, 0), 2)
        label_position = (w_min, max(h_min - 10, 10))
        cv2.putText(img, structure, label_position, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

    # Show image with annotations
    plt.figure(figsize=(6, 6))
    plt.imshow(img)
    plt.title(f"Annotations for {filename}")
    plt.axis('off')
    plt.show()

# To check if all annotations have been properly paired to the right images
import os

filtered_annotations['image_file_from_path'] = filtered_annotations['image_path'].apply(lambda x: os.path.basename(x))

# Check mismatches
mismatches = filtered_annotations[filtered_annotations['fname'] != filtered_annotations['image_file_from_path']]

# Show number of mismatches and a preview if any
print(f"Total mismatches: {len(mismatches)}")
if not mismatches.empty:
    display(mismatches.head())

import cv2
import os
import matplotlib.pyplot as plt

# Create folder to save cropped NT images
output_dir = "/content/drive/MyDrive/Thesis/Thesis Datasets/Cropped_NT"
os.makedirs(output_dir, exist_ok=True)

# Function to crop and save/display NT regions
for idx, row in filtered_annotations.iterrows():
    img = cv2.imread(row['image_path'])
    if img is None:
        print(f"Image not found: {row['image_path']}")
        continue

    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    h_min = int(row['h_min'])
    w_min = int(row['w_min'])
    h_max = int(row['h_max'])
    w_max = int(row['w_max'])

    cropped_img = img[h_min:h_max, w_min:w_max]

    # Save cropped image
    cropped_path = os.path.join(output_dir, row['fname'])
    cv2.imwrite(cropped_path, cv2.cvtColor(cropped_img, cv2.COLOR_RGB2BGR))

# Display 3 random cropped images
sample_crops = filtered_annotations.sample(3, random_state=42)
for idx, row in sample_crops.iterrows():
    cropped_path = os.path.join(output_dir, row['fname'])
    cropped_img = cv2.imread(cropped_path)
    cropped_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB)

    plt.figure(figsize=(4, 4))
    plt.imshow(cropped_img)
    plt.title(f"NT Crop: {row['fname']}")
    plt.axis('off')
    plt.show()

# Calculate NT thickness from vertical dimension (h_max - h_min)
filtered_annotations['nt_thickness_px'] = filtered_annotations['h_max'] - filtered_annotations['h_min']
filtered_annotations['nt_thickness_mm'] = filtered_annotations['nt_thickness_px'] * 0.1  # Convert to mm
filtered_annotations['label'] = filtered_annotations['nt_thickness_mm'].apply(lambda x: 1 if x > 3.5 else 0)  # 1=anomalous, 0=normal

# Calculate bounding box dimensions for cropping
filtered_annotations['width'] = filtered_annotations['w_max'] - filtered_annotations['w_min']
filtered_annotations['height'] = filtered_annotations['h_max'] - filtered_annotations['h_min']

nt_df = filtered_annotations[['image_file_from_path', 'image_path', 'nt_thickness_mm', 'label']]
nt_df.shape

nt_df.columns

nt_df.to_csv('cropped_nt.csv', index=False)

IMAGE_FOLDER_PATH = '/content/drive/MyDrive/Thesis/Thesis Datasets/Cropped_NT'

# The path to the CSV file that contains the labels.
# This is the "other dataframe" you mentioned.
LABELS_CSV_PATH = '/content/cropped_nt.csv'

# The name of the new CSV file that will contain the merged data.
OUTPUT_CSV_PATH = 'real_nt.csv'

# ==============================================================================
# 2. MERGING FUNCTION
# ==============================================================================
def join_dataframes(image_folder, labels_csv, output_path):
    """
    Creates a DataFrame of image paths and merges it with a labels DataFrame.

    Args:
        image_folder (str): Path to the directory with image files.
        labels_csv (str): Path to the CSV file containing labels.
        output_path (str): Path to save the new, merged CSV file.
    """
    print("Step 1: Reading image filenames from the folder...")
    try:
        # Get all filenames from the image folder
        image_filenames = os.listdir(image_folder)
        # Create a DataFrame with a column for the full path and a column for the filename,
        # ensuring the filename column matches the one in your labels DataFrame.
        image_df = pd.DataFrame({
            'image_path': [os.path.join(image_folder, fname) for fname in image_filenames],
            'image_file_from_path': image_filenames
        })
        print(f"Found {len(image_df)} images.")
    except FileNotFoundError:
        print(f"Error: Image folder not found at '{image_folder}'. Please check the path.")
        return

    print("\nStep 2: Loading labels from the CSV file...")
    try:
        # Read the labels DataFrame
        labels_df = pd.read_csv(labels_csv)
        print(f"Found {len(labels_df)} labels.")
    except FileNotFoundError:
        print(f"Error: Labels CSV not found at '{labels_csv}'. Please check the path.")
        return

    print("\nStep 3: Merging the two DataFrames...")
    # Perform the merge using an inner join on the 'image_file_from_path' column.
    merged_df = pd.merge(image_df, labels_df, on='image_file_from_path', how='inner')

    # Check if the merge was successful
    if merged_df.empty:
        print("Warning: The merge resulted in an empty DataFrame. This usually means the 'image_file_from_path' column in your labels CSV does not match the image filenames.")
        return

    print(f"Merge successful. The final DataFrame has {len(merged_df)} rows.")

    print("\nStep 4: Saving the final DataFrame to a new CSV file...")
    merged_df.to_csv(output_path, index=False)
    print(f"Final data saved to '{output_path}'.")

# ==============================================================================
# 3. MAIN EXECUTION BLOCK
# ==============================================================================
if __name__ == "__main__":
    join_dataframes(IMAGE_FOLDER_PATH, LABELS_CSV_PATH, OUTPUT_CSV_PATH)

import pandas as pd

nt = pd.read_csv('/content/real_nt.csv')
cropped_df = nt[['image_path_x', 'label']]

cropped_df['image_path'] = cropped_df['image_path_x']
cropped_df = cropped_df.drop(columns=['image_path_x'])

new_column_order = ['image_path', 'label']
cropped_df = cropped_df[new_column_order]

cropped_df.to_csv('real_nt.csv', index=False)

# DEFINE PATHS TO REAL AND SYTHETIC ULTRASOUND IMAGES
real_images = "/content/real_nt.csv"
synthetic_images = "/content/synthetic_df.csv"
hybrid_dataset = 'hybrid_dataset.csv'

# TARGET COUNT FOR GROUPS
TOTAL_NUM_IMAGES = 1110
TARGET_COUNT = TOTAL_NUM_IMAGES // 2
TARGET_REAL_NORMAL = int(round(TARGET_COUNT/2))
TARGET_SYNTHETIC_NORMAL = int(TARGET_COUNT - TARGET_REAL_NORMAL)
TARGET_REAL_ANOMALOUS = int(round(TARGET_COUNT/2))
TARGET_SYNTHETIC_ANOMALOUS = int(TARGET_COUNT - TARGET_REAL_ANOMALOUS)

print(TARGET_REAL_NORMAL)
print(TARGET_SYNTHETIC_NORMAL)
print(TARGET_REAL_ANOMALOUS)
print(TARGET_SYNTHETIC_ANOMALOUS)

print("Loading datasets...")
try:
    real_df = pd.read_csv(real_images)
    synthetic_df = pd.read_csv(synthetic_images)
    print("Datasets loaded successfully.")
except FileNotFoundError as e:
    print(f"Error: One of the CSV files was not found. Please check the paths.")
    print(f"Error details: {e}")
    exit()

# FILTER DATASET BY LABELS
print("Filtering data by class...")
# Real data
real_normal_df = real_df[real_df['label'] == 0].reset_index(drop=True)
real_anomalous_df = real_df[real_df['label'] == 1].reset_index(drop=True)

# Synthetic data
synthetic_normal_df = synthetic_df[synthetic_df['label'] == 0].reset_index(drop=True)
synthetic_anomalous_df = synthetic_df[synthetic_df['label'] == 1].reset_index(drop=True)

print(f"Real Normal images found: {len(real_normal_df)}")
print(f"Real Anomalous images found: {len(real_anomalous_df)}")
print(f"Synthetic Normal images found: {len(synthetic_normal_df)}")
print(f"Synthetic Anomalous images found: {len(synthetic_anomalous_df)}")

print("Sampling images to create the hybrid dataset...")

num_real_anomalous_to_sample = min(TARGET_REAL_ANOMALOUS, len(real_anomalous_df))
num_synthetic_anomalous_to_sample = TARGET_COUNT - num_real_anomalous_to_sample

num_real_normal_to_sample = min(TARGET_REAL_NORMAL, len(real_normal_df))
num_synthetic_normal_to_sample = TARGET_COUNT - num_real_normal_to_sample

print(f"Sampling {num_real_anomalous_to_sample} real anomalous images.")
print(f"Sampling {num_synthetic_anomalous_to_sample} synthetic anomalous images.")
print(f"Sampling {num_real_normal_to_sample} real normal images.")
print(f"Sampling {num_synthetic_normal_to_sample} synthetic normal images.")

# Sample the data
sampled_real_normal = real_normal_df.sample(n=TARGET_REAL_NORMAL, random_state=16, replace=False)
sampled_synthetic_normal = synthetic_normal_df.sample(n=TARGET_SYNTHETIC_NORMAL, random_state=16, replace=False)
sampled_real_anomalous = real_anomalous_df.sample(n=num_real_anomalous_to_sample, random_state=16, replace=False)
sampled_synthetic_anomalous = synthetic_anomalous_df.sample(n=num_synthetic_anomalous_to_sample, random_state=16, replace=False)

print("Combining and shuffling the dataset...")
hybrid_df = pd.concat([
    sampled_real_normal,
    sampled_synthetic_normal,
    sampled_real_anomalous,
    sampled_synthetic_anomalous
]).sample(frac=1, random_state=16).reset_index(drop=True)

print(f"New hybrid dataset created with {len(hybrid_df)} images.")
print("Final class distribution:")
print(hybrid_df['label'].value_counts())

print(f"Saving the hybrid dataset to '{hybrid_dataset}'...")
hybrid_df.to_csv(hybrid_dataset, index=False)
print("File saved successfully.")

hybrid_df = pd.read_csv('hybrid_dataset.csv')

"""# IMAGE CLASSIFICATION WITH HYBRID DATASET"""

import pandas as pd
import os
import numpy as np
import cv2
import json
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from efficientnet_pytorch import EfficientNet
import albumentations as A
from albumentations.pytorch import ToTensorV2
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, accuracy_score
from tqdm import tqdm
from torch.cuda.amp import autocast, GradScaler
from torch.optim.lr_scheduler import ReduceLROnPlateau

# set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

# Define model and training parameters
NUM_EPOCHS = 40
BATCH_SIZE = 16
LEARNING_RATE = 1e-4
SAVE_DIR = "/content/drive/MyDrive/Thesis/hybrid_model_artifacts"
os.makedirs(SAVE_DIR, exist_ok=True)
HISTORY_FILE = os.path.join(SAVE_DIR, 'hybrid_training_history.json')
MODEL_PATH = os.path.join(SAVE_DIR, 'hybrid_best_model_enet.pth')

df = pd.read_csv("/content/hybrid_dataset.csv")
type(df)

# Split into train and temp
df_train, df_temp = train_test_split(df, test_size=0.25, stratify=df['label'], random_state=16)

# Split temp into validation and test
df_val, df_test = train_test_split(df_temp, test_size=0.4, stratify=df_temp['label'], random_state=16)

def apply_clahe(image, clipLimit=2.0, tileGridSize=(8, 8)):
    """Applies Contrast Limited Adaptive Histogram Equalization (CLAHE)."""
    clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)
    return clahe.apply(image)

def get_train_transform_en():
    """Defines a robust set of augmentations for training."""
    return A.Compose([
        A.RandomResizedCrop(height=224, width=224, scale=(0.7, 1.0), ratio=(0.8, 1.2), p=0.5),
        A.Rotate(limit=15, border_mode=cv2.BORDER_CONSTANT, value=0, p=0.8),
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.3),
        A.ShiftScaleRotate(
            shift_limit=0.1,
            scale_limit=0.2,
            rotate_limit=12,
            border_mode=cv2.BORDER_CONSTANT,
            value=0,
            p=0.8
        ),
        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.7),
        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05, p=0.5),
        A.GaussianBlur(blur_limit=(3, 7), p=0.3),
        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.2),
        A.CoarseDropout(max_holes=8, max_height=16, max_width=16, fill_value=0, p=0.3),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2()
    ])

def get_val_transform_en():
    """Defines transformations for validation and test sets."""
    return A.Compose([
        A.Resize(height=256, width=256),
        A.CenterCrop(height=224, width=224),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2()
    ])

def get_test_transform_en():
    return A.Compose([
        A.Resize(height=256, width=256),
        A.CenterCrop(height=224, width=224),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2()
    ])

class HYBRIDDataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df.reset_index(drop=True)
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img_path = row['image_path']

        # Read image
        image = cv2.imread(img_path)
        if image is None:
            raise FileNotFoundError(f"Image not found at {img_path}")

        # Resize, CLAHE, and convert to RGB
        image_resized = cv2.resize(image, (224, 224))
        gray = cv2.cvtColor(image_resized, cv2.COLOR_BGR2GRAY)
        clahe_img = apply_clahe(gray)
        clahe_rgb = cv2.cvtColor(clahe_img, cv2.COLOR_GRAY2RGB)

        # Apply augmentation and normalization
        if self.transform:
            augmented = self.transform(image=clahe_rgb)
            image_tensor = augmented['image']
        else:
            image_tensor = torch.tensor(clahe_rgb).permute(2, 0, 1).float() / 255.0

        label = torch.tensor(row['label'], dtype=torch.float32)
        return image_tensor, label

train_dataset = HYBRIDDataset(df_train, transform=get_train_transform_en())
val_dataset = HYBRIDDataset(df_val, transform=get_val_transform_en())
test_dataset = HYBRIDDataset(df_test, transform=get_test_transform_en())

train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=2,
    pin_memory=True
    )

val_loader = DataLoader(
    val_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=2,
    pin_memory=True
    )

test_loader = DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=2,
    pin_memory=True
    )

    # Dataset statistics
print(f"Total samples: {len(df)}")
print(f"Training samples: {len(train_dataset)}")
print(f"Validation samples: {len(val_dataset)}")
print(f"Testing samples: {len(test_dataset)}")
print(f"Normal samples: {sum(df['label'] == 0)}")
print(f"Anomalous samples: {sum(df['label'] == 1)}")
print(f"Anomaly ratio: {sum(df['label'] == 1)/len(df):.2f}")

print("Displaying a sample image from the DataLoader...")
# Get a single batch of data from the DataLoader
images, labels = next(iter(train_loader))

# Select the first image and label from the batch
image_tensor = images[0]
label_tensor = labels[0]

# Un-normalize the image tensor for proper visualization
mean = np.array([0.485, 0.456, 0.406])
std = np.array([0.229, 0.224, 0.225])
image_np = image_tensor.permute(1, 2, 0).numpy()
image_np = std * image_np + mean
image_np = np.clip(image_np, 0, 1)

# Map the label to a descriptive string
label_map = {0.0: 'Normal', 1.0: 'Anomalous'}
label_str = label_map.get(label_tensor.item(), 'Unknown')

# Display the image and label
plt.figure(figsize=(6, 6))
plt.imshow(image_np)
plt.title(f"Image from DataLoader\nLabel: {label_str}")
plt.axis('off')
plt.show()

print("\nImage displayed successfully!")

class HYBRIDClassifier(nn.Module):
    def __init__(self):
        super(HYBRIDClassifier, self).__init__()
        self.base = EfficientNet.from_pretrained('efficientnet-b0')
        # Freeze all parameters in the base model
        for param in self.base.parameters():
            param.requires_grad = False

        # Replace the final fully connected layer with an identity module
        num_features = self.base._fc.in_features
        self.base._fc = nn.Identity()

        # Custom classifier on top of the base model
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(num_features, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Dropout(0.3),
            nn.Linear(256, 1)
        )

    def forward(self, x):
        features = self.base(x)
        return self.classifier(features)

# TRAINING HELPER FUNCTIONS
def unfreeze_layers(model, epoch):
    """Gradually unfreezes layers of the EfficientNet model for fine-tuning."""
    if epoch == 3:
        # Unfreeze classifier layers
        for param in model.classifier.parameters():
            param.requires_grad = True
        print("Unfroze classifier layers")

    elif epoch == 8:
        # Unfreeze top 3 blocks of the base model
        for name, param in model.base.named_parameters():
            if 'blocks.5' in name or 'blocks.6' in name or 'blocks.7' in name:
                param.requires_grad = True
        print("Unfroze top 3 blocks")

    elif epoch == 15:
        # Unfreeze all layers of the model
        for param in model.parameters():
            param.requires_grad = True
        print("Unfroze all layers")

# MAIN TRAINING FUNCTIONS
def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs):
    """
    Main training loop for the model.
    Saves the best model based on validation loss and training history,
    computes and stores AUC and F1-score for each epoch.
    """
    history = {
        "train_loss": [],
        "val_loss": [],
        "train_acc": [],
        "val_acc": [],
        "train_f1": [],
        "val_f1": [],
        "train_auc": [],
        "val_auc": [],
    }
    best_val_loss = float('inf')

    for epoch in range(1, num_epochs + 1):
        unfreeze_layers(model, epoch)
        print(f"Epoch {epoch}/{num_epochs}")

        # --- Training phase ---
        model.train()
        train_loss = 0.0
        train_preds = []
        train_labels = []
        train_probs = []
        total_train = 0

        for images, labels in tqdm(train_loader, desc="Training"):
            images, labels = images.to(device), labels.float().unsqueeze(1).to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * images.size(0)

            probs = torch.sigmoid(outputs)
            preds = (probs > 0.5).float()

            train_preds.extend(preds.detach().cpu().numpy().flatten())
            train_labels.extend(labels.detach().cpu().numpy().flatten())
            train_probs.extend(probs.detach().cpu().numpy().flatten())
            total_train += labels.size(0)

        avg_train_loss = train_loss / total_train
        train_accuracy = accuracy_score(train_labels, train_preds)
        train_f1 = f1_score(train_labels, train_preds)
        train_auc = roc_auc_score(train_labels, train_probs)

        history['train_loss'].append(avg_train_loss)
        history['train_acc'].append(train_accuracy)
        history['train_f1'].append(train_f1)
        history['train_auc'].append(train_auc)

        # --- Validation phase ---
        model.eval()
        val_loss = 0.0
        val_preds = []
        val_labels = []
        val_probs = []
        total_val = 0
        with torch.no_grad():
            for images, labels in tqdm(val_loader, desc="Validation"):
                images, labels = images.to(device), labels.float().unsqueeze(1).to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)

                val_loss += loss.item() * images.size(0)

                probs = torch.sigmoid(outputs)
                preds = (probs > 0.5).float()

                val_preds.extend(preds.detach().cpu().numpy().flatten())
                val_labels.extend(labels.detach().cpu().numpy().flatten())
                val_probs.extend(probs.detach().cpu().numpy().flatten())
                total_val += labels.size(0)

        avg_val_loss = val_loss / total_val
        val_accuracy = accuracy_score(val_labels, val_preds)
        val_f1 = f1_score(val_labels, val_preds)
        val_auc = roc_auc_score(val_labels, val_probs)

        history['val_loss'].append(avg_val_loss)
        history['val_acc'].append(val_accuracy)
        history['val_f1'].append(val_f1)
        history['val_auc'].append(val_auc)

        print(f"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Train F1: {train_f1:.4f}, Train AUC: {train_auc:.4f}")
        print(f"Validation Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, Val AUC: {val_auc:.4f}")

        # Save the best model based on validation loss
        if avg_val_loss < best_val_loss:
            print(f"Validation loss improved from {best_val_loss:.4f} to {avg_val_loss:.4f}. Saving model...")
            best_val_loss = avg_val_loss
            torch.save({
                'epoch': epoch,
                'hybrid_model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': best_val_loss
            }, MODEL_PATH)

    # Save training history to a JSON file
    with open(HISTORY_FILE, 'w') as f:
        json.dump(history, f)
    print("Training history saved.")

# VISUALIZATION FUNCTION
def plot_training_history(history_file):
    """Plots the training and validation loss, accuracy, F1-score, and AUC from a JSON file."""
    if not os.path.exists(history_file) or os.path.getsize(history_file) == 0:
        print(f"Error: The history file '{history_file}' does not exist or is empty.")
        return

    try:
        with open(history_file, 'r') as f:
            history = json.load(f)
    except json.JSONDecodeError:
        print(f"Error: The file '{history_file}' is not a valid JSON file. It might be corrupted.")
        return

    required_keys = ['train_loss', 'val_loss', 'train_acc', 'val_acc', 'train_f1', 'val_f1', 'train_auc', 'val_auc']
    if not all(key in history for key in required_keys):
        print("The history file is missing required data keys.")
        return

    epochs = range(1, len(history['train_loss']) + 1)

    plt.figure(figsize=(18, 12))

    # Plot Loss
    plt.subplot(2, 2, 1)
    plt.plot(epochs, history['train_loss'], 'b-o', label='Training Loss')
    plt.plot(epochs, history['val_loss'], 'r-o', label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    # Plot Accuracy
    plt.subplot(2, 2, 2)
    plt.plot(epochs, history['train_acc'], 'b-o', label='Training Accuracy')
    plt.plot(epochs, history['val_acc'], 'r-o', label='Validation Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)

    # Plot F1-Score
    plt.subplot(2, 2, 3)
    plt.plot(epochs, history['train_f1'], 'b-o', label='Training F1-Score')
    plt.plot(epochs, history['val_f1'], 'r-o', label='Validation F1-Score')
    plt.title('Training and Validation F1-Score')
    plt.xlabel('Epochs')
    plt.ylabel('F1-Score')
    plt.legend()
    plt.grid(True)

    # Plot AUC
    plt.subplot(2, 2, 4)
    plt.plot(epochs, history['train_auc'], 'b-o', label='Training AUC')
    plt.plot(epochs, history['val_auc'], 'r-o', label='Validation AUC')
    plt.title('Training and Validation AUC')
    plt.xlabel('Epochs')
    plt.ylabel('AUC')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plot_path = os.path.join(SAVE_DIR, 'hybrid_training_history_plot.png')
    plt.savefig(plot_path, dpi=300)
    plt.show()
    print(f"Training history plot saved to {plot_path}")

# VISUALIZATION OF PREDICTIONS BY THE MODEL
def visualize_predictions(results, num_samples=10):
    """
    Visualizes a selection of correct and incorrect predictions.
    """
    correct = [r for r in results if r['true_label'] == r['pred_label']]
    incorrect = [r for r in results if r['true_label'] != r['pred_label']]

    samples = (correct[:num_samples//2] if len(correct) >= num_samples//2 else correct) + \
              (incorrect[:num_samples//2] if len(incorrect) >= num_samples//2 else incorrect)

    if not samples:
        print("No samples to visualize.")
        return

    num_cols = 5
    num_rows = (len(samples) + num_cols - 1) // num_cols
    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 4 * num_rows))

    # Flatten the axes array for easier iteration
    if num_rows == 1:
        axes = [axes] if len(samples) <= 5 else axes
    else:
        axes = axes.flatten()

    for i, ax in enumerate(axes):
        if i >= len(samples):
            ax.axis('off')
            continue

        res = samples[i]

        # Load and preprocess the image
        img_path = res['image_path']
        if not os.path.exists(img_path):
            img = np.zeros((224, 224, 3), dtype=np.uint8)
            cv2.putText(img, "Dummy Image", (20, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        else:
            img = cv2.imread(img_path)
            if img is not None:
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            else:
                img = np.zeros((224, 224, 3), dtype=np.uint8)

        # Add text info
        status_map = {0.0: "NORMAL", 1.0: "ANOMALOUS"}
        true_status = status_map.get(res['true_label'], 'N/A')
        pred_status = status_map.get(res['pred_label'], 'N/A')

        color = "green" if res['true_label'] == res['pred_label'] else "red"

        ax.imshow(img)
        title = (f"True: {true_status}\nPred: {pred_status}\n"
                 f"Prob: {res['probability']:.2f}")
        ax.set_title(title, color=color, fontsize=10)
        ax.axis('off')

    plt.tight_layout()
    viz_path = os.path.join(SAVE_DIR, 'hybrid_sample_predictions.png')
    plt.savefig(viz_path, dpi=300)
    plt.show()
    print(f"Prediction visualization saved to {viz_path}")

# EVALUATION FUNCTION
def evaluate_model(model, test_loader, df_test):
    """
    Evaluates the final model on the test set and returns results for visualization.
    """
    model.eval()
    model.to(device)

    all_preds = []
    all_labels = []
    all_probs = []

    print("Evaluating model on test set...")
    with torch.no_grad():
        for images, labels in tqdm(test_loader, desc="Evaluation"):
            images, labels = images.to(device), labels.float().unsqueeze(1).to(device)
            outputs = model(images)
            probs = torch.sigmoid(outputs)
            preds = (probs > 0.5).float()

            all_preds.extend(preds.cpu().numpy().flatten())
            all_labels.extend(labels.cpu().numpy().flatten())
            all_probs.extend(probs.cpu().numpy().flatten())

    # Calculate and print metrics
    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds)
    roc_auc = roc_auc_score(all_labels, all_probs)

    print("================== Evaluation Metrics ==================")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"AUC: {roc_auc:.4f}")
    print("======================================================")

    # Plot confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(6, 6))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.colorbar()
    plt.xticks([0, 1], ['Normal', 'Anomalous'])
    plt.yticks([0, 1], ['Normal', 'Anomalous'])

    thresh = cm.max() / 2
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, format(cm[i, j], 'd'),
                     horizontalalignment="center",
                     color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()
    cm_path = os.path.join(SAVE_DIR, 'hybrid_confusion_matrix.png')
    plt.savefig(cm_path, dpi=300)
    plt.show()
    print(f"Confusion matrix saved to {cm_path}")

    # Prepare results for visualization
    test_results = []
    for i in range(len(all_labels)):
        test_results.append({
            'image_path': df_test.iloc[i]['image_path'],
            'true_label': all_labels[i],
            'pred_label': all_preds[i],
            'probability': all_probs[i]
        })
    return test_results

if __name__ == "__main__":
    # Initialize the model, optimizer, and loss function
    model = HYBRIDClassifier().to(device)
    # Freeze all layers of the pre-trained base model initially
    for param in model.base.parameters():
        param.requires_grad = False

    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.BCEWithLogitsLoss()

    # Start the training process
    train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=NUM_EPOCHS)

    # Load the best model for evaluation
    try:
        checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['hybrid_model_state_dict'])
        print("\nBest model loaded successfully for evaluation.")
    except FileNotFoundError:
        print("\nBest model not found. Cannot perform evaluation.")
        exit()

    # Evaluate the model on the test set
    test_results = evaluate_model(model, test_loader, df_test)

    # Visualize the predictions
    visualize_predictions(test_results, num_samples=10)

    # Plot the training history
    plot_training_history(HISTORY_FILE)

# GRAD-CAM
class EfficientNetGradCAM:
    """
    A class to compute and store Grad-CAM heatmaps for an EfficientNet model.
    """
    def __init__(self, model):
        self.model = model
        self.activations = None
        self.gradients = None

        # Register hooks on the last convolutional block of EfficientNet
        self.target_layer = self.model.base._blocks[-1]
        self.target_layer.register_forward_hook(self.save_activation)
        self.target_layer.register_backward_hook(self.save_gradient)

    def save_activation(self, module, input, output):
        self.activations = output

    def save_gradient(self, module, grad_input, grad_output):
        self.gradients = grad_output[0]

    def __call__(self, x):
        self.model.eval()
        self.model.zero_grad()

        # Forward pass to get output and populate activations
        output = self.model(x)

        # Backpropagate the gradient of the logit
        output.backward()

        activations = self.activations.detach()
        gradients = self.gradients.detach()

        # Pool the gradients to get weights
        weights = torch.mean(gradients, dim=[2, 3], keepdim=True)

        # Compute the CAM
        cam = torch.sum(weights * activations, dim=1, keepdim=True)
        cam = torch.relu(cam)

        # Normalize the CAM
        cam = cam - torch.min(cam)
        if torch.max(cam) > 0:
            cam = cam / torch.max(cam)
        else:
            cam = torch.zeros_like(cam)

        return cam.squeeze().cpu().numpy()

def visualize_gradcam(model, test_df, num_samples=5):
    """
    Generates and displays Grad-CAM visualizations for a few test samples.
    """
    gradcam = EfficientNetGradCAM(model)
    samples = test_df.sample(num_samples)

    # Set the model to evaluation mode
    model.eval()

    fig, axes = plt.subplots(1, num_samples, figsize=(20, 4))
    if num_samples == 1:
        axes = [axes]

    transform_for_cam = get_test_transform_en()

    for i, (_, row) in enumerate(samples.iterrows()):
        # Load and preprocess the image
        img_path = row['image_path']

        # Read image from file or create a dummy black image if not found
        if not os.path.exists(img_path):
            image = np.zeros((224, 224, 3), dtype=np.uint8)
            cv2.putText(image, "Dummy Image", (20, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        else:
            image = cv2.imread(img_path)
            if image is None:
                image = np.zeros((224, 224, 3), dtype=np.uint8)

        # Resize, CLAHE, and convert to RGB
        image_resized = cv2.resize(image, (224, 224))
        gray = cv2.cvtColor(image_resized, cv2.COLOR_BGR2GRAY)
        clahe_img = apply_clahe(gray)
        original_img_rgb = cv2.cvtColor(clahe_img, cv2.COLOR_GRAY2RGB)

        # Apply the test transform for model input
        input_tensor = transform_for_cam(image=original_img_rgb)['image'].unsqueeze(0).to(device)

        # Get prediction
        with torch.no_grad():
            output = model(input_tensor)
            prob = torch.sigmoid(output).item()
            pred = 1 if prob > 0.5 else 0

        # Get CAM - we need gradients for this part
        input_tensor.requires_grad_(True)
        cam = gradcam(input_tensor)

        # Resize CAM to original image size for overlay
        cam_resized = cv2.resize(cam, (original_img_rgb.shape[1], original_img_rgb.shape[0]))
        heatmap = (cam_resized * 255).astype(np.uint8)
        heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

        # Alpha blend the heatmap and original image
        superimposed_img = cv2.addWeighted(original_img_rgb, 0.6, heatmap, 0.4, 0)

        # Plot the result
        axes[i].imshow(superimposed_img)
        status_map = {0.0: "NORMAL", 1.0: "ANOMALOUS"}
        true_status = status_map.get(row['label'], 'N/A')
        pred_status = status_map.get(pred, 'N/A')

        color = "green" if row['label'] == pred else "red"

        title = (f"True: {true_status} | Pred: {pred_status}\n"
                 f"Prob: {prob:.2f}")
        axes[i].set_title(title, color=color, fontsize=10)
        axes[i].axis('off')

    plt.tight_layout()
    viz_path = os.path.join(SAVE_DIR, 'hybrid_efficientnet_gradcam.png')
    plt.savefig(viz_path, dpi=300)
    plt.show()
    print(f"Grad-CAM visualization saved to {viz_path}")

# --- Visualize Grad-CAM ---
if __name__ == "__main__":
    model = HYBRIDClassifier().to(device)

    try:
        checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['hybrid_model_state_dict'])
        print("Best model loaded successfully for Grad-CAM visualization.")
    except FileNotFoundError:
        print("Best model not found. Cannot perform Grad-CAM visualization.")
        exit()

    visualize_gradcam(model, df_test, num_samples=5)

"""# COMPARATIVE EVALUATION OF ALL DATA CONFIGURATIONS"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import os
import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_curve, roc_auc_score, confusion_matrix,classification_report
from torch.utils.data import Dataset, DataLoader
import umap
from efficientnet_pytorch import EfficientNet

# DEFINE MODEL PATHS
BATCH_SIZE = 16
df_test = pd.read_csv('df_test.csv')
MODEL_PATHS = {
    'Real': '/content/drive/MyDrive/Thesis/real_model_artifacts/real_best_model_enet.pth',
    'Synthetic': '/content/drive/MyDrive/Thesis/synthetic_model_artifacts2/syn_best_model_enet.pth',
    'Hybrid': '/content/drive/MyDrive/Thesis/hybrid_model_artifacts/hybrid_best_model_enet.pth'
}

def apply_clahe(image, clipLimit=2.0, tileGridSize=(8, 8)):
    """Applies Contrast Limited Adaptive Histogram Equalization (CLAHE)."""
    clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)
    return clahe.apply(image)

def get_test_transform_en():
    return A.Compose([
        A.Resize(height=256, width=256),
        A.CenterCrop(height=224, width=224),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2()
    ])

# DATA LOADER
class Dataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df.reset_index(drop=True)
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img_path = row['image_path']

        # Read image
        image = cv2.imread(img_path)
        if image is None:
            raise FileNotFoundError(f"Image not found at {img_path}")

        # Resize, CLAHE, and convert to RGB
        image_resized = cv2.resize(image, (224, 224))
        gray = cv2.cvtColor(image_resized, cv2.COLOR_BGR2GRAY)
        clahe_img = apply_clahe(gray)
        clahe_rgb = cv2.cvtColor(clahe_img, cv2.COLOR_GRAY2RGB)

        # Apply augmentation and normalization
        if self.transform:
            augmented = self.transform(image=clahe_rgb)
            image_tensor = augmented['image']
        else:
            image_tensor = torch.tensor(clahe_rgb).permute(2, 0, 1).float() / 255.0

        label = torch.tensor(row['label'], dtype=torch.float32)
        return image_tensor, label

test_dataset = Dataset(df_test, transform=get_test_transform_en())
test_loader = DataLoader(
    test_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=2,
    pin_memory=True
    )

class Classifier(nn.Module):
    def __init__(self):
        super(Classifier, self).__init__()
        self.base = EfficientNet.from_pretrained('efficientnet-b0')

        # Replace the final fully connected layer with an identity module
        num_features = self.base._fc.in_features
        self.base._fc = nn.Identity()

        # Custom classifier on top of the base model
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(num_features, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Dropout(0.3),
            nn.Linear(256, 1)
        )

    def forward(self, x):
        features = self.base(x)
        return self.classifier(features)

MODEL_CLASS = Classifier

# Helper function to load a model and its state dictionary
def load_model(path, model_class, device, model_name):
    """
    Loads a PyTorch model from a .pth file, handling common checkpoint formats.
    """
    if not os.path.exists(path):
        print(f"Error: Model file not found at '{path}'. Skipping.")
        return None
    try:
        model = model_class()
        checkpoint = torch.load(path, map_location=device)

        # Define a list of possible keys for the state dictionary
        possible_keys = [
            f"{model_name.lower()}_model_state_dict",
            "state_dict",
            "real_model_state_dict", # Fallback for your specific real model
            "syn_model_state_dict",  # Fallback for your specific synthetic model
            "hybrid_model_state_dict" # Fallback for your specific hybrid model
        ]

        state_dict = None
        for key in possible_keys:
            if key in checkpoint:
                state_dict = checkpoint[key]
                print(f"Found model state dictionary under key: '{key}'")
                break

        # If no key is found, assume the checkpoint itself is the state_dict
        if state_dict is None:
            state_dict = checkpoint
            print("Assuming checkpoint is the state dictionary itself.")

        model.load_state_dict(state_dict)
        model.to(device)
        model.eval()
        print(f"Model loaded from '{path}' successfully.")
        return model
    except Exception as e:
        print(f"Error loading model from '{path}': {e}. Skipping.")
        return None

def evaluate_and_plot(model, dataloader, device, model_name, results, y_test_global, y_probas_global):
    """
    Evaluates a PyTorch model, stores its results, and generates a confusion matrix.
    """
    if model is None:
        print(f"\n--- Skipping evaluation for '{model_name}' ---")
        return

    print(f"\n--- Evaluating Model: {model_name} ---")

    all_labels = []
    all_preds = []
    all_probas = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            probas = torch.sigmoid(outputs).squeeze()
            preds = (probas > 0.5).long()

            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())
            all_probas.extend(probas.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)
    recall = recall_score(all_labels, all_preds, average='binary', zero_division=0)
    f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0)

    try:
        auc = roc_auc_score(all_labels, all_probas)
    except ValueError:
        auc = 0.0

    conf_matrix = confusion_matrix(all_labels, all_preds)

    results[model_name] = {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,

        'f1': f1,
        'auc': auc,
        'confusion_matrix': conf_matrix
    }

    y_test_global[model_name] = all_labels
    y_probas_global[model_name] = all_probas

    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")
    print(f"AUC: {auc:.4f}")
    print("\nClassification Report:")
    print(classification_report(all_labels, all_preds, zero_division=0))

    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues",
                xticklabels=['Normal', 'Anomalous'],
                yticklabels=['Normal', 'Anomalous'])
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title(f'Confusion Matrix for {model_name}')
    plt.savefig(f'conf_matrix {model_name}', dpi=300)
    plt.show()
    return results, y_test_global, y_probas_global

def plot_comparative_metrics(results):
    """Generates a bar chart to compare key metrics across models."""
    fig, ax = plt.subplots(figsize=(12, 8))
    metrics_to_plot = ['accuracy', 'f1', 'auc']
    model_names = list(results.keys())

    x = np.arange(len(model_names))
    width = 0.25

    for i, metric in enumerate(metrics_to_plot):
        values = [results[name][metric] for name in model_names]
        ax.bar(x + i*width, values, width, label=metric.upper())

    ax.set_ylabel('Score')
    ax.set_title('Comparative Performance Metrics')
    ax.set_xticks(x + width)
    ax.set_xticklabels(model_names)
    ax.legend()
    plt.ylim(0.0, 1.0)
    plt.show()

def plot_roc_curves(y_test_global, y_probas_global):
    """Generates a single plot with ROC curves for all models."""
    plt.figure(figsize=(10, 8))

    for model_name, y_test in y_test_global.items():
        y_probas = y_probas_global[model_name]
        fpr, tpr, _ = roc_curve(y_test, y_probas)
        auc = roc_auc_score(y_test, y_probas)
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.4f})')

    plt.plot([0, 1], [0, 1], 'k--', label='Random Guessing')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc='lower right')
    plt.show()

def save_results_to_csv(results_dict, path):
    """Saves evaluation results to a CSV file."""
    # Convert the dictionary to a DataFrame
    df = pd.DataFrame.from_dict(results_dict, orient='index')

    # Drop the confusion matrix as it's not a single value
    df = df.drop(columns=['confusion_matrix'])

    # Round the numeric columns for cleaner output
    df = df.round(4)

    # Save to CSV
    df.to_csv(path)
    print(f"\nResults saved successfully to: {path}")

if __name__ == "__main__":
    model_results = {}
    y_test_global = {}
    y_probas_global = {}

    for model_name, model_path in MODEL_PATHS.items():
        model = load_model(model_path, MODEL_CLASS, device, model_name)
        evaluate_and_plot(model, test_loader, device, model_name, model_results, y_test_global, y_probas_global)

    if model_results:
        # Save the results to a CSV file
        results_save_path = '/content/drive/MyDrive/Thesis/evaluation_results.csv'
        save_results_to_csv(model_results, results_save_path)

        print("\n--- Generating Comparative Plots ---")
        plot_comparative_metrics(model_results)
        plot_roc_curves(y_test_global, y_probas_global)

"""**UMAP VISUALIZATION**"""

# Number of samples to use for UMAP visualization
NUM_SAMPLES_PER_DATASET = 200

def get_features(model, dataloader, device, num_samples):
    """
    Extracts features from the penultimate layer of the model for a given dataset.
    This function will be called for each model (Real, Synthetic, Hybrid) using
    the same real test dataset as input. The features returned will represent
    how each respective model 'views' the real data.
    """
    model.eval()
    features_list = []

    with torch.no_grad():
        for i, (inputs, _) in enumerate(dataloader):
            if len(features_list) >= num_samples:
                break
            inputs = inputs.to(device)
            # The model's forward pass should return features from the base
            # to do this, we'll manually get the output of the base network
            features = model.base(inputs)
            features_list.append(features.cpu().numpy())

    return np.vstack(features_list)

def plot_umap(features, labels, title, filename):
    """
    Generates a UMAP plot for the given features and labels.
    """
    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, metric='euclidean', random_state=16)
    embedding = reducer.fit_transform(features)

    plt.figure(figsize=(12, 10))
    sns.scatterplot(
        x=embedding[:, 0],
        y=embedding[:, 1],
        hue=labels,
        palette=sns.color_palette("hsv", len(np.unique(labels))),
        legend="full",
        s=50,
        alpha=0.7
    )
    plt.title(title, fontsize=20)
    plt.xlabel('UMAP Dimension 1', fontsize=14)
    plt.ylabel('UMAP Dimension 2', fontsize=14)
    plt.savefig('UMAP Viz', dpi=300)
    plt.show()

if __name__ == "__main__":
    # Create an empty dictionary to store features from each model
    model_features = {}

    # Extract features using each model
    for model_name, model_path in MODEL_PATHS.items():
        print(f"Loading and extracting features with the '{model_name}' model...")
        model = Classifier()
        try:
            checkpoint = torch.load(model_path, map_location=device)

            # This is the new, more specific logic to handle your saved files.
            state_dict_key = f"{model_name.lower()}_model_state_dict"
            # Explicitly check for the "syn_model_state_dict" key as a fallback
            if state_dict_key in checkpoint:
                state_dict = checkpoint[state_dict_key]
            elif "syn_model_state_dict" in checkpoint and model_name == "Synthetic":
                state_dict = checkpoint["syn_model_state_dict"]
            elif "model_state_dict" in checkpoint:
                 state_dict = checkpoint["model_state_dict"]
            else:
                 # If the key is not found, assume the checkpoint itself is the state_dict
                 state_dict = checkpoint


            model.load_state_dict(state_dict, strict=False)
            model.to(device)
            model.eval()

            # Pass the same real test data to each model to get features
            features = get_features(model, test_loader, device, NUM_SAMPLES_PER_DATASET)

            # Only add features if they were successfully extracted
            if features.size > 0:
                model_features[model_name] = features
                print(f"Extracted {features.shape[0]} features with the '{model_name}' model.")
            else:
                print(f"No features extracted for model '{model_name}'.")

        except Exception as e:
            print(f"Error loading model '{model_name}': {e}. Skipping.")

    # Combine features and labels for UMAP
    if model_features:
        all_features = np.vstack(list(model_features.values()))
        all_labels = []
        for model_name, features in model_features.items():
            all_labels.extend([model_name] * features.shape[0])

        print(f"\nPerforming UMAP on {len(all_labels)} combined samples.")

        # Generate and plot UMAP
        if all_features.shape[0] > 0:
            plot_umap(all_features, all_labels, 'UMAP Visualization of Real, Synthetic, and Hybrid Data', 'umap_plot.png')
        else:
            print("No features were extracted. Please check model paths and dataset.")
    else:
        print("\nNo models were successfully loaded or no features could be extracted. Please check the paths to your model files and your dataset.")